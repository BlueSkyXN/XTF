#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„ç”µå­è¡¨æ ¼APIæ¨¡å—
é’ˆå¯¹4ç§åŒæ­¥æ¨¡å¼è®¾è®¡æœ€ä¼˜æ¥å£é€‰æ‹©ç­–ç•¥
"""

import logging
import time
from typing import Dict, Any, List, Optional, Tuple

from .auth import FeishuAuth
from .base import RetryableAPIClient
from core.config import SyncMode


class OptimizedSheetAPI:
    """ä¼˜åŒ–çš„é£ä¹¦ç”µå­è¡¨æ ¼APIå®¢æˆ·ç«¯"""
    
    def __init__(self, auth: FeishuAuth, api_client: Optional[RetryableAPIClient] = None):
        """åˆå§‹åŒ–ä¼˜åŒ–çš„ç”µå­è¡¨æ ¼APIå®¢æˆ·ç«¯"""
        self.auth = auth
        self.api_client = api_client or auth.api_client
        self.logger = logging.getLogger(__name__)
        
        # APIé™åˆ¶å¸¸é‡
        self.MAX_ROWS_PER_CALL = 5000
        self.MAX_COLS_PER_CALL = 100
        self.MAX_BATCH_RANGES = 10  # batch_updateæœ€å¤§èŒƒå›´æ•°ï¼ˆç»éªŒå€¼ï¼‰
        self.ERROR_CODE_REQUEST_TOO_LARGE = 90227
        
    def sync_data(self, spreadsheet_token: str, sheet_id: str, values: List[List[Any]],
                  sync_mode: SyncMode, row_batch_size: int = 500,
                  col_batch_size: int = 80, rate_limit_delay: float = 0.3) -> bool:
        """
        æ™ºèƒ½åŒæ­¥æ•°æ® - æ ¹æ®åŒæ­¥æ¨¡å¼é€‰æ‹©æœ€ä¼˜APIç­–ç•¥
        
        Args:
            spreadsheet_token: ç”µå­è¡¨æ ¼Token
            sheet_id: å·¥ä½œè¡¨ID
            values: è¦åŒæ­¥çš„æ•°æ®ï¼ˆç¬¬ä¸€è¡Œä¸ºè¡¨å¤´ï¼‰
            sync_mode: åŒæ­¥æ¨¡å¼
            row_batch_size: è¡Œæ‰¹æ¬¡å¤§å°
            col_batch_size: åˆ—æ‰¹æ¬¡å¤§å°
            rate_limit_delay: æ¥å£è°ƒç”¨é—´éš”
            
        Returns:
            æ˜¯å¦åŒæ­¥æˆåŠŸ
        """
        if not values:
            self.logger.warning("åŒæ­¥æ•°æ®ä¸ºç©º")
            return True
            
        total_rows = len(values)
        total_cols = len(values[0]) if values else 0
        
        self.logger.info(f"ğŸš€ å¼€å§‹æ™ºèƒ½åŒæ­¥: {total_rows} è¡Œ Ã— {total_cols} åˆ—")
        self.logger.info(f"ğŸ“‹ åŒæ­¥æ¨¡å¼: {sync_mode.value}")
        self.logger.info(f"âš™ï¸  åˆå§‹æ‰¹æ¬¡é…ç½®: {row_batch_size} è¡Œ/æ‰¹ Ã— {col_batch_size} åˆ—/æ‰¹")
        
        # æ ¹æ®åŒæ­¥æ¨¡å¼é€‰æ‹©æœ€ä¼˜ç­–ç•¥
        if sync_mode == SyncMode.CLONE:
            return self._sync_clone_optimized(spreadsheet_token, sheet_id, values,
                                            row_batch_size, col_batch_size, rate_limit_delay)
        elif sync_mode == SyncMode.INCREMENTAL:
            return self._sync_incremental_optimized(spreadsheet_token, sheet_id, values, 
                                                  row_batch_size, col_batch_size, rate_limit_delay)
        elif sync_mode == SyncMode.OVERWRITE:
            return self._sync_overwrite_optimized(spreadsheet_token, sheet_id, values, 
                                                row_batch_size, col_batch_size, rate_limit_delay)
        else:  # FULL
            return self._sync_full_optimized(spreadsheet_token, sheet_id, values, 
                                           row_batch_size, col_batch_size, rate_limit_delay)
    
    def _sync_clone_optimized(self, spreadsheet_token: str, sheet_id: str, values: List[List[Any]],
                             row_batch_size: int, col_batch_size: int, rate_limit_delay: float) -> bool:
        """
        å…‹éš†åŒæ­¥ä¼˜åŒ–ç­–ç•¥ï¼š
        1. æ ¹æ®åˆå§‹é…ç½®åˆ›å»ºæ•°æ®å—ã€‚
        2. é€ä¸ªä¸Šä¼ æ•°æ®å—ã€‚
        3. å¦‚æœé‡åˆ°â€œè¯·æ±‚è¿‡å¤§â€çš„é”™è¯¯ï¼Œåˆ™è‡ªåŠ¨å°†è¯¥å—å¯¹åŠåˆ†å‰²å¹¶é€’å½’é‡è¯•ã€‚
        """
        self.logger.info("ğŸ”„ æ‰§è¡Œå…‹éš†åŒæ­¥ä¼˜åŒ–ç­–ç•¥ (å…·å¤‡è‡ªåŠ¨äºŒåˆ†é‡è¯•èƒ½åŠ›)")
        
        data_chunks = self._create_data_chunks(values, row_batch_size, col_batch_size)
        total_chunks = len(data_chunks)
        
        self.logger.info(f"ğŸ“¦ åˆå§‹æ•°æ®åˆ†å—å®Œæˆ: å…± {total_chunks} ä¸ªæ•°æ®å—")

        for i, chunk in enumerate(data_chunks, 1):
            self.logger.info(f"--- å¼€å§‹å¤„ç†åˆå§‹æ•°æ®å— {i}/{total_chunks} ---")
            if not self._upload_chunk_with_auto_split(spreadsheet_token, sheet_id, chunk, rate_limit_delay):
                self.logger.error(f"âŒ åˆå§‹æ•°æ®å— {i}/{total_chunks} (è¡Œ {chunk['start_row']}-{chunk['end_row']}) æœ€ç»ˆä¸Šä¼ å¤±è´¥")
                return False
            self.logger.info(f"--- âœ… æˆåŠŸå¤„ç†åˆå§‹æ•°æ®å— {i}/{total_chunks} ---")
            
        self.logger.info(f"ğŸ‰ å…‹éš†åŒæ­¥å…¨éƒ¨å®Œæˆ: æˆåŠŸå¤„ç† {total_chunks} ä¸ªåˆå§‹æ•°æ®å—")
        return True
    
    def _sync_incremental_optimized(self, spreadsheet_token: str, sheet_id: str, values: List[List[Any]], 
                                   row_batch_size: int, col_batch_size: int, rate_limit_delay: float) -> bool:
        """
        å¢é‡åŒæ­¥ä¼˜åŒ–ç­–ç•¥ï¼šä½¿ç”¨ append æ¥å£çš„ INSERT_ROWS æ¨¡å¼
        ä¼˜åŠ¿ï¼šè‡ªåŠ¨æ’å…¥è¡Œï¼Œç¡®ä¿ä¸è¦†ç›–ç°æœ‰æ•°æ®
        """
        self.logger.info("â• æ‰§è¡Œå¢é‡åŒæ­¥ä¼˜åŒ–ç­–ç•¥ (ä½¿ç”¨ append + INSERT_ROWS)")
        
        # å»æ‰è¡¨å¤´ï¼Œåªè¿½åŠ æ•°æ®è¡Œ
        data_rows = values[1:] if len(values) > 1 else []
        if not data_rows:
            self.logger.info("æ— æ•°æ®è¡Œéœ€è¦è¿½åŠ ")
            return True
        
        return self._append_data_with_insert_rows(spreadsheet_token, sheet_id, data_rows, 
                                                row_batch_size, col_batch_size, rate_limit_delay)
    
    def _sync_overwrite_optimized(self, spreadsheet_token: str, sheet_id: str, values: List[List[Any]], 
                                 row_batch_size: int, col_batch_size: int, rate_limit_delay: float) -> bool:
        """
        è¦†ç›–åŒæ­¥ä¼˜åŒ–ç­–ç•¥ï¼šä½¿ç”¨ PUT values ç›´æ¥è¦†ç›–
        ä¼˜åŠ¿ï¼šç®€å•ç›´æ¥ï¼Œç²¾ç¡®æ§åˆ¶è¦†ç›–èŒƒå›´
        """
        self.logger.info("ğŸ”„ æ‰§è¡Œè¦†ç›–åŒæ­¥ä¼˜åŒ–ç­–ç•¥ (ä½¿ç”¨ PUT values)")
        
        return self._write_data_direct_overwrite(spreadsheet_token, sheet_id, values, 
                                               row_batch_size, col_batch_size, rate_limit_delay)
    
    def _sync_full_optimized(self, spreadsheet_token: str, sheet_id: str, values: List[List[Any]], 
                            row_batch_size: int, col_batch_size: int, rate_limit_delay: float) -> bool:
        """
        å…¨é‡åŒæ­¥ä¼˜åŒ–ç­–ç•¥ï¼šæ··åˆä½¿ç”¨ PUT values + append
        ä¼˜åŠ¿ï¼šæ›´æ–°ç°æœ‰æ•°æ®ç²¾ç¡®ï¼Œè¿½åŠ æ–°æ•°æ®é«˜æ•ˆ
        """
        self.logger.info("ğŸ”„ æ‰§è¡Œå…¨é‡åŒæ­¥ä¼˜åŒ–ç­–ç•¥ (æ··åˆ PUT + append)")
        
        # è¿™é‡Œéœ€è¦ç°æœ‰æ•°æ®å¯¹æ¯”é€»è¾‘ï¼Œæš‚æ—¶ä½¿ç”¨å…‹éš†åŒæ­¥ç­–ç•¥
        return self._sync_clone_optimized(spreadsheet_token, sheet_id, values, 
                                        row_batch_size, col_batch_size, rate_limit_delay)
    
    def _create_data_chunks(self, values: List[List[Any]], row_batch_size: int, col_batch_size: int) -> List[Dict]:
        """
        åˆ›å»ºæ•°æ®åˆ†å—
        
        Returns:
            åŒ…å«åˆ†å—ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ï¼š
            - data: æ•°æ®å—
            - start_row, end_row: è¡ŒèŒƒå›´
            - start_col, end_col: åˆ—èŒƒå›´  
        """
        chunks = []
        total_rows = len(values)
        total_cols = len(values[0]) if values else 0
        
        # æŒ‰åˆ—åˆ†å—ï¼ˆå¤–å±‚å¾ªç¯ï¼‰
        for col_start in range(0, total_cols, col_batch_size):
            col_end = min(col_start + col_batch_size, total_cols)
            
            # æŒ‰è¡Œåˆ†å—ï¼ˆå†…å±‚å¾ªç¯ï¼‰
            for row_start in range(0, total_rows, row_batch_size):
                row_end = min(row_start + row_batch_size, total_rows)
                
                # æå–æ•°æ®å—
                chunk_data = []
                for row_idx in range(row_start, row_end):
                    if row_idx < len(values):
                        chunk_row = values[row_idx][col_start:col_end]
                        # ç¡®ä¿è¡Œé•¿åº¦ä¸åˆ—å—å¤§å°ä¸€è‡´
                        while len(chunk_row) < (col_end - col_start):
                            chunk_row.append("")
                        chunk_data.append(chunk_row)
                
                if chunk_data:  # åªæ·»åŠ éç©ºå—
                    chunks.append({
                        'data': chunk_data,
                        'start_row': row_start + 1,  # 1-based indexing
                        'end_row': row_start + len(chunk_data),
                        'start_col': col_start + 1,  # 1-based indexing  
                        'end_col': col_end
                    })
        
        return chunks

    def _build_range_string(self, sheet_id: str, start_row: int, start_col: int, end_row: int, end_col: int) -> str:
        """æ„å»ºèŒƒå›´å­—ç¬¦ä¸²"""
        start_col_letter = self._column_number_to_letter(start_col)
        end_col_letter = self._column_number_to_letter(end_col)
        return f"{sheet_id}!{start_col_letter}{start_row}:{end_col_letter}{end_row}"
    
    def _column_number_to_letter(self, col_num: int) -> str:
        """å°†åˆ—å·è½¬æ¢ä¸ºå­—æ¯"""
        result = ""
        while col_num > 0:
            col_num -= 1
            result = chr(65 + col_num % 26) + result
            col_num //= 26
        return result or "A"
    
    def _upload_chunk_with_auto_split(self, spreadsheet_token: str, sheet_id: str, chunk: Dict, rate_limit_delay: float) -> bool:
        """
        ä¸Šä¼ å•ä¸ªæ•°æ®å—ï¼Œå¦‚æœå› è¯·æ±‚è¿‡å¤§å¤±è´¥ï¼Œåˆ™è‡ªåŠ¨äºŒåˆ†é‡è¯•ã€‚
        """
        # å‡†å¤‡è¯·æ±‚æ•°æ®
        range_str = self._build_range_string(sheet_id, chunk['start_row'], chunk['start_col'], chunk['end_row'], chunk['end_col'])
        value_ranges = [{"range": range_str, "values": chunk['data']}]
        
        self.logger.info(f"ğŸ“¤ å°è¯•ä¸Šä¼ : {len(chunk['data'])} è¡Œ (èŒƒå›´ {range_str})")

        # å‘èµ·APIè°ƒç”¨
        success, error_code = self._batch_update_ranges(spreadsheet_token, value_ranges)
        
        if success:
            self.logger.info(f"âœ… ä¸Šä¼ æˆåŠŸ: {len(chunk['data'])} è¡Œ")
            # æˆåŠŸä¸Šä¼ åè¿›è¡Œé¢‘ç‡æ§åˆ¶
            if rate_limit_delay > 0:
                time.sleep(rate_limit_delay)
            return True
            
        # å¦‚æœå¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯è¯·æ±‚è¿‡å¤§é”™è¯¯
        if error_code == self.ERROR_CODE_REQUEST_TOO_LARGE:
            num_rows = len(chunk['data'])
            self.logger.warning(f"æ£€æµ‹åˆ°è¯·æ±‚è¿‡å¤§é”™è¯¯ (é”™è¯¯ç  {error_code})ï¼Œå½“å‰å—åŒ…å« {num_rows} è¡Œï¼Œå°†è¿›è¡ŒäºŒåˆ†ã€‚")

            # å¦‚æœå—å·²ç»å°åˆ°æ— æ³•å†åˆ†ï¼Œåˆ™è§†ä¸ºæœ€ç»ˆå¤±è´¥
            if num_rows <= 1:
                self.logger.error(f"âŒ å—å¤§å°å·²ä¸º {num_rows} è¡Œï¼Œæ— æ³•å†åˆ†å‰²ï¼Œä¸Šä¼ å¤±è´¥ã€‚")
                return False

            # å°†å½“å‰å—åˆ†å‰²æˆä¸¤ä¸ªå­å—
            mid_point = num_rows // 2
            
            chunk1_data = chunk['data'][:mid_point]
            chunk1 = {
                'data': chunk1_data,
                'start_row': chunk['start_row'],
                'end_row': chunk['start_row'] + len(chunk1_data) - 1,
                'start_col': chunk['start_col'],
                'end_col': chunk['end_col']
            }

            chunk2_data = chunk['data'][mid_point:]
            chunk2 = {
                'data': chunk2_data,
                'start_row': chunk['start_row'] + mid_point,
                'end_row': chunk['start_row'] + mid_point + len(chunk2_data) - 1,
                'start_col': chunk['start_col'],
                'end_col': chunk['end_col']
            }
            
            # é€’å½’ä¸Šä¼ ä¸¤ä¸ªå­å—
            self.logger.info(f" åˆ†å‰²ä¸º: å—1 ({len(chunk1_data)}è¡Œ), å—2 ({len(chunk2_data)}è¡Œ)")
            return (self._upload_chunk_with_auto_split(spreadsheet_token, sheet_id, chunk1, rate_limit_delay) and
                    self._upload_chunk_with_auto_split(spreadsheet_token, sheet_id, chunk2, rate_limit_delay))

        # å…¶ä»–ç±»å‹çš„APIé”™è¯¯ï¼Œç›´æ¥åˆ¤ä¸ºå¤±è´¥
        self.logger.error(f"âŒ ä¸Šä¼ å‘ç”Ÿä¸å¯æ¢å¤çš„é”™è¯¯ (é”™è¯¯ç : {error_code})")
        return False

    def _batch_update_ranges(self, spreadsheet_token: str, value_ranges: List[Dict]) -> Tuple[bool, Optional[int]]:
        """
        æ‰¹é‡æ›´æ–°å¤šä¸ªèŒƒå›´ã€‚

        Returns:
            å…ƒç»„ (æ˜¯å¦æˆåŠŸ, é”™è¯¯ç )
        """
        url = f"https://open.feishu.cn/open-apis/sheets/v2/spreadsheets/{spreadsheet_token}/values_batch_update"
        headers = self.auth.get_auth_headers()
        
        data = {"valueRanges": value_ranges}
        
        response = self.api_client.call_api("POST", url, headers=headers, json=data)
        
        try:
            result = response.json()
        except ValueError as e:
            self.logger.error(f"æ‰¹é‡å†™å…¥å“åº”è§£æå¤±è´¥: {e}, HTTPçŠ¶æ€ç : {response.status_code}")
            return False, None
        
        code = result.get("code")
        if code != 0:
            error_msg = result.get('msg', 'æœªçŸ¥é”™è¯¯')
            self.logger.error(f"æ‰¹é‡å†™å…¥å¤±è´¥: é”™è¯¯ç  {code}, é”™è¯¯ä¿¡æ¯: {error_msg}")
            self.logger.debug(f"APIå“åº”: {result}")
            return False, code
        
        # è®°å½•è¯¦ç»†çš„å†™å…¥ç»“æœ
        responses = result.get("data", {}).get("responses", [])
        total_cells = sum(resp.get("updatedCells", 0) for resp in responses)
        self.logger.debug(f"æ‰¹é‡å†™å…¥æˆåŠŸ: {len(responses)} ä¸ªèŒƒå›´, å…± {total_cells} ä¸ªå•å…ƒæ ¼")
        
        return True, 0
    
    def _append_data_with_insert_rows(self, spreadsheet_token: str, sheet_id: str, data_rows: List[List[Any]], 
                                     row_batch_size: int, col_batch_size: int, rate_limit_delay: float) -> bool:
        """
        ä½¿ç”¨INSERT_ROWSæ¨¡å¼è¿½åŠ æ•°æ®
        """
        total_rows = len(data_rows)
        total_cols = len(data_rows[0]) if data_rows else 0
        
        success_count = 0
        batch_count = 0
        
        # æŒ‰åˆ—åˆ†å—
        for col_start in range(0, total_cols, col_batch_size):
            col_end = min(col_start + col_batch_size, total_cols)
            
            # æŒ‰è¡Œåˆ†å—
            for row_start in range(0, total_rows, row_batch_size):
                row_end = min(row_start + row_batch_size, total_rows)
                
                # æå–æ‰¹æ¬¡æ•°æ®
                batch_data = []
                for row_idx in range(row_start, row_end):
                    chunk_row = data_rows[row_idx][col_start:col_end]
                    while len(chunk_row) < (col_end - col_start):
                        chunk_row.append("")
                    batch_data.append(chunk_row)
                
                if not batch_data:
                    continue
                
                batch_count += 1
                
                # æ„å»ºè¿½åŠ èŒƒå›´
                start_col_letter = self._column_number_to_letter(col_start + 1)
                end_col_letter = self._column_number_to_letter(col_end)
                append_range = f"{sheet_id}!{start_col_letter}:{end_col_letter}"
                
                self.logger.info(f"ğŸ“¤ è¿½åŠ æ‰¹æ¬¡ {batch_count}: è¡Œ {row_start + 1}-{row_end}, "
                               f"åˆ— {start_col_letter}-{end_col_letter} ({len(batch_data)} è¡Œ)")
                
                if self._append_single_batch_with_insert(spreadsheet_token, append_range, batch_data):
                    success_count += 1
                    self.logger.info(f"âœ… æ‰¹æ¬¡ {batch_count} è¿½åŠ æˆåŠŸ")
                else:
                    self.logger.error(f"âŒ æ‰¹æ¬¡ {batch_count} è¿½åŠ å¤±è´¥")
                    return False
                
                # é¢‘ç‡æ§åˆ¶
                if rate_limit_delay > 0:
                    time.sleep(rate_limit_delay)
        
        self.logger.info(f"ğŸ‰ å¢é‡åŒæ­¥å®Œæˆ: æˆåŠŸè¿½åŠ  {success_count}/{batch_count} ä¸ªæ‰¹æ¬¡")
        return success_count == batch_count
    
    def _append_single_batch_with_insert(self, spreadsheet_token: str, range_str: str, values: List[List[Any]]) -> bool:
        """
        ä½¿ç”¨INSERT_ROWSæ¨¡å¼è¿½åŠ å•ä¸ªæ‰¹æ¬¡
        """
        url = f"https://open.feishu.cn/open-apis/sheets/v2/spreadsheets/{spreadsheet_token}/values_append?insertDataOption=INSERT_ROWS"
        headers = self.auth.get_auth_headers()
        
        data = {
            "valueRange": {
                "range": range_str,
                "values": values
            }
        }
        
        response = self.api_client.call_api("POST", url, headers=headers, json=data)
        
        try:
            result = response.json()
        except ValueError as e:
            self.logger.error(f"è¿½åŠ æ•°æ®å“åº”è§£æå¤±è´¥: {e}")
            return False
        
        if result.get("code") != 0:
            error_msg = result.get('msg', 'æœªçŸ¥é”™è¯¯')
            self.logger.error(f"è¿½åŠ æ•°æ®å¤±è´¥: é”™è¯¯ç  {result.get('code')}, é”™è¯¯ä¿¡æ¯: {error_msg}")
            return False
        
        return True
    
    def _write_data_direct_overwrite(self, spreadsheet_token: str, sheet_id: str, values: List[List[Any]], 
                                    row_batch_size: int, col_batch_size: int, rate_limit_delay: float) -> bool:
        """
        ç›´æ¥è¦†ç›–å†™å…¥æ•°æ®
        """
        # ä½¿ç”¨å…‹éš†ç­–ç•¥çš„åˆ†å—é€»è¾‘ï¼Œä½†ç”¨PUTæ¥å£
        data_chunks = self._create_data_chunks(values, row_batch_size, col_batch_size)
        total_chunks = len(data_chunks)
        
        success_count = 0
        
        for i, chunk in enumerate(data_chunks, 1):
            range_str = self._build_range_string(sheet_id, chunk['start_row'], 
                                               chunk['start_col'], chunk['end_row'], chunk['end_col'])
            
            self.logger.info(f"ğŸ“¤ è¦†ç›–æ‰¹æ¬¡ {i}: è¡Œ {chunk['start_row']}-{chunk['end_row']}, "
                           f"åˆ— {self._column_number_to_letter(chunk['start_col'])}-{self._column_number_to_letter(chunk['end_col'])} "
                           f"({len(chunk['data'])} è¡Œ)")
            
            if self._write_single_range(spreadsheet_token, range_str, chunk['data']):
                success_count += 1
                self.logger.info(f"âœ… æ‰¹æ¬¡ {i} è¦†ç›–æˆåŠŸ")
            else:
                self.logger.error(f"âŒ æ‰¹æ¬¡ {i} è¦†ç›–å¤±è´¥")
                return False
            
            # é¢‘ç‡æ§åˆ¶
            if rate_limit_delay > 0:
                time.sleep(rate_limit_delay)
        
        self.logger.info(f"ğŸ‰ è¦†ç›–åŒæ­¥å®Œæˆ: æˆåŠŸè¦†ç›– {success_count}/{total_chunks} ä¸ªæ‰¹æ¬¡")
        return success_count == total_chunks
    
    def _write_single_range(self, spreadsheet_token: str, range_str: str, values: List[List[Any]]) -> bool:
        """
        å†™å…¥å•ä¸ªèŒƒå›´
        """
        url = f"https://open.feishu.cn/open-apis/sheets/v2/spreadsheets/{spreadsheet_token}/values"
        headers = self.auth.get_auth_headers()
        
        data = {
            "valueRange": {
                "range": range_str,
                "values": values
            }
        }
        
        response = self.api_client.call_api("PUT", url, headers=headers, json=data)
        
        try:
            result = response.json()
        except ValueError as e:
            self.logger.error(f"å†™å…¥æ•°æ®å“åº”è§£æå¤±è´¥: {e}")
            return False
        
        if result.get("code") != 0:
            error_msg = result.get('msg', 'æœªçŸ¥é”™è¯¯')
            self.logger.error(f"å†™å…¥æ•°æ®å¤±è´¥: é”™è¯¯ç  {result.get('code')}, é”™è¯¯ä¿¡æ¯: {error_msg}")
            return False
        
        return True
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€åŒæ­¥å¼•æ“æ¨¡å—

æ¨¡å—æ¦‚è¿°ï¼š
    æ­¤æ¨¡å—å®ç°äº† XTF å·¥å…·çš„æ ¸å¿ƒåŒæ­¥é€»è¾‘ï¼Œæä¾›ç»Ÿä¸€çš„åŒæ­¥å¼•æ“ç±»
    XTFSyncEngineï¼Œæ”¯æŒå¤šç»´è¡¨æ ¼ï¼ˆBitableï¼‰å’Œç”µå­è¡¨æ ¼ï¼ˆSheetï¼‰
    ä¸¤ç§ç›®æ ‡ç±»å‹çš„æ•°æ®åŒæ­¥ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
    1. ç»Ÿä¸€çš„åŒæ­¥å…¥å£å’Œæµç¨‹æ§åˆ¶
    2. å¤šç»´è¡¨æ ¼å­—æ®µç®¡ç†ï¼ˆè‡ªåŠ¨åˆ›å»ºç¼ºå¤±å­—æ®µï¼‰
    3. å››ç§åŒæ­¥æ¨¡å¼çš„å…·ä½“å®ç°
    4. æ‰¹é‡æ•°æ®å¤„ç†ä¸åˆ†å—ä¸Šä¼ 
    5. é€‰æ‹©æ€§åˆ—åŒæ­¥æ”¯æŒ
    6. æ—¥å¿—ç³»ç»Ÿé…ç½®å’Œç®¡ç†
    7. å…¨å±€è¯·æ±‚æ§åˆ¶å™¨é›†æˆ

æ ¸å¿ƒç±»ï¼š
    XTFSyncEngine:
        ç»Ÿä¸€åŒæ­¥å¼•æ“ï¼Œæ ¹æ®é…ç½®çš„ç›®æ ‡ç±»å‹è‡ªåŠ¨é€‰æ‹©å¯¹åº”çš„ API å®¢æˆ·ç«¯
        å’ŒåŒæ­¥ç­–ç•¥ï¼Œæ‰§è¡Œæ•°æ®åŒæ­¥æ“ä½œã€‚

åŒæ­¥æ¨¡å¼è¯´æ˜ï¼š
    - fullï¼ˆå…¨é‡åŒæ­¥ï¼‰ï¼š
        å¯¹æ¯”ç´¢å¼•åˆ—ï¼Œå·²å­˜åœ¨çš„è®°å½•æ›´æ–°ï¼Œä¸å­˜åœ¨çš„æ–°å¢
        
    - incrementalï¼ˆå¢é‡åŒæ­¥ï¼‰ï¼š
        ä»…æ–°å¢æœ¬åœ°æœ‰è€Œè¿œç¨‹æ²¡æœ‰çš„è®°å½•ï¼Œè·³è¿‡å·²å­˜åœ¨è®°å½•
        
    - overwriteï¼ˆè¦†ç›–åŒæ­¥ï¼‰ï¼š
        å…ˆåˆ é™¤è¿œç¨‹è¡¨ä¸­ä¸æœ¬åœ°æ•°æ®ç´¢å¼•åŒ¹é…çš„è®°å½•ï¼Œå†æ–°å¢å…¨éƒ¨æœ¬åœ°æ•°æ®
        
    - cloneï¼ˆå…‹éš†åŒæ­¥ï¼‰ï¼š
        æ¸…ç©ºè¿œç¨‹è¡¨å…¨éƒ¨æ•°æ®ï¼Œç„¶åå®Œæ•´å†™å…¥æœ¬åœ°æ•°æ®

åŒæ­¥æµç¨‹ï¼š
    1. åˆå§‹åŒ–æ—¥å¿—å’Œ API å®¢æˆ·ç«¯
    2. è·å–/åˆ›å»ºè¿œç¨‹è¡¨å­—æ®µï¼ˆBitableï¼‰
    3. è·å–è¿œç¨‹ç°æœ‰æ•°æ®
    4. æ ¹æ®åŒæ­¥æ¨¡å¼æ‰§è¡Œç›¸åº”æ“ä½œ
    5. æ‰¹é‡å¤„ç†æ•°æ®ï¼ˆåˆ†å—ã€é‡è¯•ã€é”™è¯¯å¤„ç†ï¼‰
    6. è¿”å›åŒæ­¥ç»“æœ

ä¾èµ–å…³ç³»ï¼š
    å†…éƒ¨æ¨¡å—ï¼š
        - core.config: é…ç½®ç±»ï¼ˆSyncConfig, SyncMode, TargetTypeï¼‰
        - core.converter: æ•°æ®è½¬æ¢ï¼ˆDataConverterï¼‰
        - api: APIå®¢æˆ·ç«¯ï¼ˆFeishuAuth, BitableAPI, SheetAPIç­‰ï¼‰
    å¤–éƒ¨ä¾èµ–ï¼š
        - pandas: æ•°æ®å¤„ç†
        - logging: æ—¥å¿—è®°å½•

æ€§èƒ½ä¼˜åŒ–ï¼š
    1. æ‰¹é‡æ“ä½œå‡å°‘ API è°ƒç”¨æ¬¡æ•°
    2. é¢„åˆ†å—æœºåˆ¶åº”å¯¹å¤§æ•°æ®é‡
    3. æ™ºèƒ½é‡è¯•å’Œé¢‘æ§ç­–ç•¥
    4. é€‰æ‹©æ€§åˆ—åŒæ­¥å‡å°‘æ•°æ®ä¼ è¾“

é”™è¯¯å¤„ç†ï¼š
    - ä¸‰å±‚æ•°æ®ä¸Šä¼ ä¿æŠ¤æœºåˆ¶
    - è‡ªåŠ¨äºŒåˆ†é‡è¯•ï¼ˆé’ˆå¯¹è¯·æ±‚è¿‡å¤§é”™è¯¯ï¼‰
    - è¯¦ç»†çš„é”™è¯¯æ—¥å¿—å’ŒçŠ¶æ€åé¦ˆ

ä½¿ç”¨ç¤ºä¾‹ï¼š
    >>> from core.config import SyncConfig, TargetType
    >>> from core.engine import XTFSyncEngine
    >>> 
    >>> config = SyncConfig(...)
    >>> engine = XTFSyncEngine(config)
    >>> success = engine.sync(dataframe)

æ³¨æ„äº‹é¡¹ï¼š
    1. åŒæ­¥å‰ä¼šè‡ªåŠ¨è®¾ç½®æ—¥å¿—ï¼Œæ—¥å¿—æ–‡ä»¶ä¿å­˜åœ¨ logs/ ç›®å½•
    2. Bitable æ¨¡å¼æ”¯æŒè‡ªåŠ¨åˆ›å»ºç¼ºå¤±å­—æ®µ
    3. å¤§æ•°æ®é‡å»ºè®®è°ƒæ•´ batch_size å‚æ•°
    4. clone æ¨¡å¼ä¼šæ¸…ç©ºè¿œç¨‹è¡¨ï¼Œè¯·è°¨æ…ä½¿ç”¨

ä½œè€…: XTF Team
ç‰ˆæœ¬: 1.7.3+
æ›´æ–°æ—¥æœŸ: 2026-01-24
"""

import pandas as pd
import time
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List, Union, Tuple

from .config import SyncConfig, SyncMode, TargetType
from .converter import DataConverter
from api import FeishuAuth, RetryableAPIClient, BitableAPI, SheetAPI, RateLimiter


class XTFSyncEngine:
    """ç»Ÿä¸€åŒæ­¥å¼•æ“ - æ”¯æŒå¤šç»´è¡¨æ ¼å’Œç”µå­è¡¨æ ¼"""

    def __init__(self, config: SyncConfig):
        """
        åˆå§‹åŒ–åŒæ­¥å¼•æ“

        Args:
            config: ç»Ÿä¸€åŒæ­¥é…ç½®å¯¹è±¡
        """
        self.config = config

        # è®¾ç½®æ—¥å¿—ï¼ˆå¿…é¡»å…ˆè®¾ç½®ï¼Œå› ä¸ºå…¶ä»–åˆå§‹åŒ–å¯èƒ½éœ€è¦æ—¥å¿—ï¼‰
        self.setup_logging()
        self.logger = logging.getLogger("XTF.engine")

        # åˆå§‹åŒ–å…¨å±€è¯·æ±‚æ§åˆ¶å™¨ï¼ˆå¦‚æœé…ç½®äº†é«˜çº§é‡è¯•å’Œé¢‘æ§ç­–ç•¥ï¼‰
        self._init_global_controller()

        # åˆå§‹åŒ–APIç»„ä»¶
        self.auth = FeishuAuth(config.app_id, config.app_secret)
        self.api_client = RetryableAPIClient(
            max_retries=config.max_retries,
            rate_limiter=RateLimiter(config.rate_limit_delay),
        )

        # æ ¹æ®ç›®æ ‡ç±»å‹é€‰æ‹©APIå®¢æˆ·ç«¯
        self.api: Union[BitableAPI, SheetAPI]
        if config.target_type == TargetType.BITABLE:
            self.api = BitableAPI(self.auth, self.api_client)
        else:  # SHEET
            self.api = SheetAPI(
                self.auth,
                self.api_client,
                start_row=self.config.start_row,
                start_column=self.config.start_column,
                scan_max_rows=self.config.sheet_scan_max_rows,
                scan_max_cols=self.config.sheet_scan_max_cols,
                write_max_rows=self.config.sheet_write_max_rows,
                write_max_cols=self.config.sheet_write_max_cols,
                value_render_option=self.config.sheet_value_render_option,
                datetime_render_option=self.config.sheet_datetime_render_option,
            )
        # åˆå§‹åŒ–æ•°æ®è½¬æ¢å™¨
        self.converter = DataConverter(config.target_type)
        # ç¼“å­˜å·¥ä½œè¡¨ç½‘æ ¼å±æ€§ï¼Œé¿å…é‡å¤è¯·æ±‚
        self._sheet_grid_cache: Optional[Tuple[int, int]] = None
        self._sheet_grid_cache_key: Optional[Tuple[str, str]] = None

    def _init_global_controller(self):
        """åˆå§‹åŒ–å…¨å±€è¯·æ±‚æ§åˆ¶å™¨"""
        try:
            from .config import ConfigManager

            # ä½¿ç”¨é…ç½®ç®¡ç†å™¨åˆ›å»ºå…¨å±€æ§åˆ¶å™¨
            global_controller = ConfigManager.create_request_controller(self.config)
            if global_controller:
                self.logger.info(
                    f"å·²åˆå§‹åŒ–å…¨å±€è¯·æ±‚æ§åˆ¶å™¨ - é‡è¯•ç­–ç•¥: {self.config.retry_strategy_type}, "
                    f"é¢‘æ§ç­–ç•¥: {self.config.rate_limit_strategy_type}"
                )
            else:
                self.logger.info(
                    f"ä½¿ç”¨ä¼ ç»Ÿæ§åˆ¶æ¨¡å¼ - é‡è¯•æ¬¡æ•°: {self.config.max_retries}, "
                    f"é¢‘æ§é—´éš”: {self.config.rate_limit_delay}s"
                )
        except Exception as e:
            self.logger.warning(f"åˆå§‹åŒ–å…¨å±€æ§åˆ¶å™¨å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ¨¡å¼: {e}")

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)

        target_name = (
            "bitable" if self.config.target_type == TargetType.BITABLE else "sheet"
        )
        log_file = (
            log_dir
            / f"xtf_{target_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        )

        # è·å–XTFä¸“ç”¨çš„loggerï¼Œé¿å…å…¨å±€æ±¡æŸ“
        xtf_logger = logging.getLogger("XTF")
        xtf_logger.handlers.clear()

        level = getattr(logging, self.config.log_level.upper(), logging.INFO)
        xtf_logger.setLevel(level)

        # è®¾ç½®æ ¼å¼
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )

        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file, encoding="utf-8")
        file_handler.setFormatter(formatter)
        xtf_logger.addHandler(file_handler)

        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        xtf_logger.addHandler(console_handler)

        # é˜²æ­¢ä¼ æ’­åˆ°æ ¹logger
        xtf_logger.propagate = False

    # ========== å¤šç»´è¡¨æ ¼ä¸“ç”¨æ–¹æ³• ==========

    def get_field_types(self) -> Dict[str, int]:
        """è·å–å¤šç»´è¡¨æ ¼å­—æ®µç±»å‹æ˜ å°„"""
        if self.config.target_type != TargetType.BITABLE:
            return {}

        try:
            if not isinstance(self.api, BitableAPI):
                return {}
            if not self.config.app_token or not self.config.table_id:
                self.logger.error("å¤šç»´è¡¨æ ¼çš„ app_token æˆ– table_id æœªé…ç½®")
                return {}
            existing_fields = self.api.list_fields(
                self.config.app_token, self.config.table_id
            )
            field_types = {}
            for field in existing_fields:
                field_name = field.get("field_name", "")
                field_type = field.get("type", 1)  # é»˜è®¤ä¸ºæ–‡æœ¬ç±»å‹
                field_types[field_name] = field_type

            self.logger.debug(f"è·å–åˆ° {len(field_types)} ä¸ªå­—æ®µç±»å‹ä¿¡æ¯")
            return field_types

        except Exception as e:
            self.logger.warning(f"è·å–å­—æ®µç±»å‹å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æ™ºèƒ½ç±»å‹æ£€æµ‹")
            return {}

    def ensure_fields_exist(self, df: pd.DataFrame) -> Tuple[bool, Dict[str, int]]:
        """ç¡®ä¿å¤šç»´è¡¨æ ¼æ‰€éœ€å­—æ®µå­˜åœ¨"""
        if self.config.target_type != TargetType.BITABLE:
            return True, {}

        try:
            if not isinstance(self.api, BitableAPI):
                return False, {}
            if not self.config.app_token or not self.config.table_id:
                self.logger.error("å¤šç»´è¡¨æ ¼çš„ app_token æˆ– table_id æœªé…ç½®")
                return False, {}

            # è·å–ç°æœ‰å­—æ®µ
            existing_fields = self.api.list_fields(
                self.config.app_token, self.config.table_id
            )
            existing_field_names = {field["field_name"] for field in existing_fields}

            # æ„å»ºå­—æ®µç±»å‹æ˜ å°„
            field_types = {}
            for field in existing_fields:
                field_name = field.get("field_name", "")
                field_type = field.get("type", 1)
                field_types[field_name] = field_type

            if self.config.create_missing_fields:
                # æ‰¾å‡ºç¼ºå¤±çš„å­—æ®µï¼Œä¿æŒåŸå§‹åˆ—é¡ºåº
                required_fields = set(df.columns)
                missing_fields_set = required_fields - existing_field_names

                # æŒ‰ç…§ DataFrame åˆ—çš„åŸå§‹é¡ºåºæ’åˆ—ç¼ºå¤±å­—æ®µ
                missing_fields = [
                    col for col in df.columns if col in missing_fields_set
                ]

                if missing_fields:
                    self.logger.info(f"æ£€æµ‹åˆ° {len(missing_fields)} ä¸ªç¼ºå¤±å­—æ®µ")
                    self.logger.info(
                        f"ä½¿ç”¨å­—æ®µç±»å‹ç­–ç•¥: {self.config.field_type_strategy.value}"
                    )

                    # åˆ†ææ¯ä¸ªç¼ºå¤±å­—æ®µ
                    creation_plan = []
                    for field_name in missing_fields:
                        # ä½¿ç”¨å¢å¼ºçš„åˆ†ææ–¹æ³•
                        analysis = self.converter.analyze_excel_column_data_enhanced(
                            df,
                            field_name,
                            self.config.field_type_strategy.value,
                            self.config,
                        )

                        creation_plan.append(
                            {
                                "field_name": field_name,
                                "suggested_type": analysis["suggested_feishu_type"],
                                "confidence": analysis["confidence"],
                                "reason": analysis["recommendation_reason"],
                                "has_validation": analysis["has_excel_validation"],
                            }
                        )

                    # æ˜¾ç¤ºåˆ›å»ºè®¡åˆ’
                    self.logger.info("=" * 60)
                    self.logger.info("ğŸ“‹ å­—æ®µåˆ›å»ºè®¡åˆ’:")
                    for plan in creation_plan:
                        validation_mark = "ğŸ“‹" if plan["has_validation"] else "ğŸ“"
                        self.logger.info(
                            f"{validation_mark} {plan['field_name']}: "
                            f"{self.converter.get_field_type_name(plan['suggested_type'])} "
                            f"(ç½®ä¿¡åº¦: {plan['confidence']:.1%}) - {plan['reason']}"
                        )
                    self.logger.info("=" * 60)

                    # æ‰§è¡Œå­—æ®µåˆ›å»º
                    for plan in creation_plan:
                        if not isinstance(self.api, BitableAPI):
                            continue
                        success = self.api.create_field(
                            self.config.app_token,
                            self.config.table_id,
                            plan["field_name"],
                            plan["suggested_type"],
                        )

                        if not success:
                            self.logger.error(f"å­—æ®µ '{plan['field_name']}' åˆ›å»ºå¤±è´¥")
                            return False, field_types

                        # è®°å½•æ–°å­—æ®µç±»å‹
                        field_types[plan["field_name"]] = plan["suggested_type"]

                    # ç­‰å¾…å­—æ®µåˆ›å»ºå®Œæˆ
                    import time

                    time.sleep(2)

                else:
                    self.logger.info("âœ… æ‰€æœ‰å¿…éœ€å­—æ®µå·²å­˜åœ¨ï¼Œæ— éœ€åˆ›å»º")

            return True, field_types

        except Exception as e:
            self.logger.error(f"å­—æ®µæ£€æŸ¥å¤±è´¥: {e}")
            return False, {}

    def get_all_bitable_records(
        self, field_names: Optional[List[str]] = None
    ) -> List[Dict]:
        """è·å–æ‰€æœ‰å¤šç»´è¡¨æ ¼è®°å½•

        Args:
            field_names: æŒ‡å®šè¿”å›çš„å­—æ®µåç§°åˆ—è¡¨ï¼Œä¸ºNoneæ—¶è¿”å›å…¨éƒ¨å­—æ®µã€‚
                         ç”¨äºå‡å°‘ä¸å¿…è¦çš„æ•°æ®ä¼ è¾“ï¼Œæå‡æŸ¥è¯¢æ€§èƒ½ã€‚
        """
        if not isinstance(self.api, BitableAPI):
            return []
        if not self.config.app_token or not self.config.table_id:
            self.logger.error("å¤šç»´è¡¨æ ¼çš„ app_token æˆ– table_id æœªé…ç½®")
            return []
        return self.api.get_all_records(
            self.config.app_token, self.config.table_id, field_names=field_names
        )

    def process_in_batches(
        self, items: List[Any], batch_size: int, processor_func, *args, **kwargs
    ) -> bool:
        """åˆ†æ‰¹å¤„ç†æ•°æ®ï¼ˆå¤šç»´è¡¨æ ¼æ¨¡å¼ï¼‰"""
        if self.config.target_type != TargetType.BITABLE:
            return False

        # æŒ‰æ¥å£ä¸Šé™è‡ªåŠ¨é™åˆ¶æ‰¹å¤§å°ï¼Œé¿å…è¶…é™è¯·æ±‚
        max_batch_size = self._get_operation_max_batch_size(processor_func)
        effective_batch_size = batch_size
        if max_batch_size and batch_size > max_batch_size:
            self.logger.warning(
                f"{self._get_operation_type(processor_func)}æ‰¹å¤„ç†å¤§å° {batch_size} è¶…è¿‡æ¥å£ä¸Šé™ {max_batch_size}ï¼Œå·²è‡ªåŠ¨é™è‡³ {max_batch_size}"
            )
            effective_batch_size = max_batch_size

        total_batches = (len(items) + effective_batch_size - 1) // effective_batch_size
        success_count = 0

        # è·å–æ“ä½œç±»å‹ç”¨äºæ—¥å¿—æ˜¾ç¤º
        operation_type = self._get_operation_type(processor_func)

        for i in range(0, len(items), effective_batch_size):
            batch = items[i : i + effective_batch_size]
            batch_num = i // effective_batch_size + 1
            start_row = i + 1  # Excelè¡Œå·ä»1å¼€å§‹
            end_row = min(i + len(batch), len(items))

            try:
                # ä¿®å¤å‚æ•°ä¼ é€’é¡ºåºï¼šå…ˆä¼ é€’å›ºå®šå‚æ•°ï¼Œå†ä¼ é€’æ‰¹æ¬¡æ•°æ®
                if processor_func(*args, batch, **kwargs):
                    success_count += 1
                    # æ˜¾ç¤ºå…·ä½“çš„è¡ŒèŒƒå›´ä¿¡æ¯
                    range_info = (
                        f"ç¬¬{start_row}-{end_row}è¡Œ"
                        if start_row != end_row
                        else f"ç¬¬{start_row}è¡Œ"
                    )
                    self.logger.info(
                        f"âœ… {operation_type}æˆåŠŸ: æ‰¹æ¬¡{batch_num}/{total_batches}, {len(batch)}æ¡è®°å½• ({range_info})"
                    )
                else:
                    self.logger.error(
                        f"âŒ {operation_type}å¤±è´¥: æ‰¹æ¬¡{batch_num}/{total_batches}"
                    )
            except Exception as e:
                self.logger.error(
                    f"âŒ {operation_type}å¼‚å¸¸: æ‰¹æ¬¡{batch_num}/{total_batches}, é”™è¯¯: {e}"
                )

        self.logger.info(
            f"ğŸ‰ {operation_type}å®Œæˆ: {success_count}/{total_batches} ä¸ªæ‰¹æ¬¡æˆåŠŸ"
        )
        return success_count == total_batches

    def _get_operation_type(self, processor_func) -> str:
        """æ ¹æ®å¤„ç†å‡½æ•°è·å–æ“ä½œç±»å‹"""
        func_name = getattr(processor_func, "__name__", str(processor_func))
        if "create" in func_name:
            return "æ‰¹é‡åˆ›å»º"
        elif "update" in func_name:
            return "æ‰¹é‡æ›´æ–°"
        elif "delete" in func_name:
            return "æ‰¹é‡åˆ é™¤"
        else:
            return "æ‰¹é‡å¤„ç†"

    def _get_operation_max_batch_size(self, processor_func) -> Optional[int]:
        """æ ¹æ®å¤„ç†å‡½æ•°è·å–æ‰¹é‡æ¥å£ä¸Šé™"""
        func_name = getattr(processor_func, "__name__", str(processor_func))
        if "create" in func_name:
            return BitableAPI.MAX_BATCH_CREATE_SIZE
        if "update" in func_name:
            return BitableAPI.MAX_BATCH_UPDATE_SIZE
        if "delete" in func_name:
            return BitableAPI.MAX_BATCH_DELETE_SIZE
        return None

    # ========== ç”µå­è¡¨æ ¼ä¸“ç”¨æ–¹æ³• ==========

    def _get_sheet_grid_properties(self) -> Optional[Tuple[int, int]]:
        """è·å–å·¥ä½œè¡¨ç½‘æ ¼å±æ€§ï¼ˆè¡Œæ•°ã€åˆ—æ•°ï¼‰"""
        if self.config.target_type != TargetType.SHEET:
            return None
        if not isinstance(self.api, SheetAPI):
            return None
        if not self.config.spreadsheet_token or not self.config.sheet_id:
            return None
        cache_key = (self.config.spreadsheet_token, self.config.sheet_id)
        if self._sheet_grid_cache_key == cache_key and self._sheet_grid_cache:
            return self._sheet_grid_cache
        try:
            grid = self.api.get_sheet_grid_properties(
                self.config.spreadsheet_token, self.config.sheet_id
            )
            self._sheet_grid_cache = grid
            self._sheet_grid_cache_key = cache_key
            return grid
        except Exception as e:
            self.logger.warning(f"è·å–å·¥ä½œè¡¨ç½‘æ ¼å±æ€§å¤±è´¥: {e}")
            return None

    def _build_sheet_full_range(self) -> Optional[str]:
        """æ„å»ºè¦†ç›–æ•´ä¸ªå·¥ä½œè¡¨çš„èŒƒå›´å­—ç¬¦ä¸²ï¼ˆåŸºäºç½‘æ ¼å±æ€§ï¼‰"""
        grid = self._get_sheet_grid_properties()
        if not grid:
            return None
        row_count, col_count = grid
        if row_count <= 0 or col_count <= 0:
            return None
        if not isinstance(self.api, SheetAPI):
            return None
        end_col = self.api.column_number_to_letter(col_count)
        return f"A1:{end_col}{row_count}"

    def get_current_sheet_data(self) -> pd.DataFrame:
        """è·å–å½“å‰ç”µå­è¡¨æ ¼æ•°æ®"""
        if self.config.target_type != TargetType.SHEET:
            return pd.DataFrame()

        # æ„å»ºä»é…ç½®èµ·å§‹ç‚¹å¼€å§‹çš„è¯»å–èŒƒå›´
        start_cell = f"{self.config.start_column}{self.config.start_row}"
        read_range = None
        end_row = None
        end_col = None

        # ä¼˜å…ˆä½¿ç”¨å·¥ä½œè¡¨ç½‘æ ¼å±æ€§ç²¾ç¡®é™å®šèŒƒå›´
        grid = self._get_sheet_grid_properties()
        if grid and isinstance(self.api, SheetAPI):
            row_count, col_count = grid
            start_col_num = self.api.column_letter_to_number(
                self.config.start_column
            )
            if row_count < self.config.start_row or col_count < start_col_num:
                self.logger.info(
                    f"å·¥ä½œè¡¨ç½‘æ ¼èŒƒå›´å°äºèµ·å§‹ä½ç½®: "
                    f"row_count={row_count}, column_count={col_count}, "
                    f"start={start_cell}"
                )
                return pd.DataFrame()

            end_row = row_count
            end_col = self.api.column_number_to_letter(col_count)
            read_range = (
                f"{self.config.sheet_id}!"
                f"{self.config.start_column}{self.config.start_row}:{end_col}{end_row}"
            )
        else:
            # å…œåº•ï¼šä½¿ç”¨å†å²é»˜è®¤èŒƒå›´ï¼ˆæ³¨æ„ï¼šå¯èƒ½è¾ƒå¤§ï¼‰
            end_row = 500000
            end_col = "ZZ"
            read_range = f"{self.config.sheet_id}!{start_cell}:{end_col}{end_row}"
            self.logger.warning(
                "æ— æ³•è·å–å·¥ä½œè¡¨ç½‘æ ¼å±æ€§ï¼Œé€€å›é»˜è®¤è¯»å–èŒƒå›´ï¼Œå¯èƒ½è¾ƒå¤§"
            )

        self.logger.info(f"å°è¯•ä»èŒƒå›´è¯»å–æ•°æ®: {read_range}")

        try:
            if not isinstance(self.api, SheetAPI):
                return pd.DataFrame()
            if not self.config.spreadsheet_token:
                self.logger.error("ç”µå­è¡¨æ ¼çš„ spreadsheet_token æœªé…ç½®")
                return pd.DataFrame()

            if not (end_row and end_col):
                return pd.DataFrame()

            values = self.api.get_sheet_data_chunked(
                self.config.spreadsheet_token,
                self.config.sheet_id,
                self.config.start_row,
                end_row,
                self.config.start_column,
                end_col,
            )
            df = self.converter.values_to_df(values)

            if not df.empty:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆæ•°æ®ï¼ˆè‡³å°‘æœ‰ä¸€è¡Œæ•°æ®åŒ…å«éç©ºå€¼ï¼‰
                has_valid_data = False
                for _, row in df.iterrows():
                    if any(pd.notnull(val) and str(val).strip() != "" for val in row):
                        has_valid_data = True
                        break

                if has_valid_data:
                    self.logger.info(
                        f"æˆåŠŸè·å–ç”µå­è¡¨æ ¼æ•°æ®: {len(df)} è¡Œ x {len(df.columns)} åˆ— (ä» {start_cell} å¼€å§‹)"
                    )
                    return df

            # å¦‚æœdfä¸ºç©ºæˆ–æ•°æ®å…¨ä¸ºç©ºï¼Œè¯´æ˜è¡¨æ ¼åœ¨æŒ‡å®šèŒƒå›´ç¡®å®æ˜¯ç©ºçš„
            self.logger.info(f"åœ¨èŒƒå›´ {read_range} å†…æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè§†ä¸ºç©ºè¡¨")
            return pd.DataFrame()

        except Exception as e:
            self.logger.warning(f"å°è¯•ä»èŒƒå›´ {read_range} è¯»å–æ•°æ®å¤±è´¥: {e}")
            self.logger.warning("æ— æ³•è·å–ç”µå­è¡¨æ ¼æ•°æ®ï¼Œå°†ä½¿ç”¨è¦†ç›–æ¨¡å¼")
            return pd.DataFrame()

    def get_sheet_data_with_validation(
        self
    ) -> tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[set]]:
        """
        è·å–ç”µå­è¡¨æ ¼æ•°æ®ï¼ˆæ”¯æŒåŒè¯»ç”¨äºç»“æœæ£€æµ‹ï¼‰

        Returns:
            (result_df, formula_df, formula_columns):
            - result_df: è®¡ç®—ç»“æœæ•°æ®ï¼ˆç”¨äºæ¯”è¾ƒï¼‰
            - formula_df: å…¬å¼æ•°æ®ï¼ˆä»…åœ¨å¯ç”¨ validate_results æ—¶è¿”å›ï¼‰
            - formula_columns: åŒ…å«å…¬å¼çš„åˆ—é›†åˆï¼ˆåˆ—åï¼‰
        """
        if not self.config.sheet_validate_results:
            # æœªå¯ç”¨æ£€æµ‹ï¼Œä½¿ç”¨åŸæœ‰å•æ¬¡è¯»å–é€»è¾‘
            return self.get_current_sheet_data(), None, None

        # å¯ç”¨æ£€æµ‹ï¼Œæ‰§è¡ŒåŒè¯»
        if not isinstance(self.api, SheetAPI):
            return pd.DataFrame(), None, None

        if not self.config.spreadsheet_token or not self.config.sheet_id:
            return pd.DataFrame(), None, None

        # è·å–ç½‘æ ¼èŒƒå›´
        grid = self._get_sheet_grid_properties()
        if not grid:
            self.logger.warning("æ— æ³•è·å–å·¥ä½œè¡¨ç½‘æ ¼å±æ€§ï¼Œæ— æ³•è¿›è¡ŒåŒè¯»")
            return self.get_current_sheet_data(), None, None

        row_count, col_count = grid
        start_col_num = self.api.column_letter_to_number(self.config.start_column)
        if row_count < self.config.start_row or col_count < start_col_num:
            self.logger.info("å·¥ä½œè¡¨èŒƒå›´å°äºèµ·å§‹ä½ç½®ï¼Œè§†ä¸ºç©ºè¡¨")
            return pd.DataFrame(), None, None

        end_row = row_count
        end_col = self.api.column_number_to_letter(col_count)

        self.logger.info("ğŸ” å¯ç”¨ç»“æœæ£€æµ‹ï¼Œå¼€å§‹åŒè¯»äº‘ç«¯æ•°æ®...")

        # ç¬¬ä¸€æ¬¡è¯»å–ï¼šå…¬å¼æ¨¡å¼
        self.logger.info("  ğŸ“– è¯»å–å…¬å¼æ•°æ®...")
        try:
            # ä¸´æ—¶è®¾ç½®è¯»å–é€‰é¡¹ä¸º Formula
            original_value_option = self.config.sheet_value_render_option
            original_datetime_option = self.config.sheet_datetime_render_option

            # å¼ºåˆ¶ä½¿ç”¨ Formula æ¨¡å¼è¯»å–
            self.config.sheet_value_render_option = "Formula"
            self.config.sheet_datetime_render_option = None

            formula_values = self.api.get_sheet_data_chunked(
                self.config.spreadsheet_token,
                self.config.sheet_id,
                self.config.start_row,
                end_row,
                self.config.start_column,
                end_col,
            )
            formula_df = self.converter.values_to_df(formula_values)

            # æ¢å¤åŸæœ‰é…ç½®
            self.config.sheet_value_render_option = original_value_option
            self.config.sheet_datetime_render_option = original_datetime_option

        except Exception as e:
            self.logger.warning(f"è¯»å–å…¬å¼æ•°æ®å¤±è´¥: {e}")
            return self.get_current_sheet_data(), None, None

        # ç¬¬äºŒæ¬¡è¯»å–ï¼šç»“æœæ¨¡å¼
        self.logger.info("  ğŸ“Š è¯»å–è®¡ç®—ç»“æœæ•°æ®...")
        try:
            # ä½¿ç”¨é…ç½®çš„è¯»å–é€‰é¡¹ï¼ˆæˆ– FormattedValue ä½œä¸ºé»˜è®¤ï¼‰
            if not self.config.sheet_value_render_option:
                self.config.sheet_value_render_option = "FormattedValue"
            if not self.config.sheet_datetime_render_option:
                self.config.sheet_datetime_render_option = "FormattedString"

            result_values = self.api.get_sheet_data_chunked(
                self.config.spreadsheet_token,
                self.config.sheet_id,
                self.config.start_row,
                end_row,
                self.config.start_column,
                end_col,
            )
            result_df = self.converter.values_to_df(result_values)

        except Exception as e:
            self.logger.warning(f"è¯»å–ç»“æœæ•°æ®å¤±è´¥: {e}")
            return self.get_current_sheet_data(), None, None

        # è¯†åˆ«å…¬å¼åˆ—
        if formula_df.empty:
            formula_columns = set()
        else:
            # è½¬æ¢ä¸ºäºŒç»´åˆ—è¡¨ç”¨äºè¯†åˆ«
            formula_data = [formula_df.columns.tolist()] + formula_df.values.tolist()
            formula_columns = self.api.identify_formula_columns(
                formula_data, headers=formula_df.columns.tolist()
            )

        if formula_columns:
            self.logger.info(f"  ğŸ”’ è¯†åˆ«åˆ°å…¬å¼åˆ—: {sorted(formula_columns)}")
        else:
            self.logger.info("  â„¹ï¸  æœªè¯†åˆ«åˆ°å…¬å¼åˆ—")

        return result_df, formula_df, formula_columns

    def validate_and_report_differences(
        self,
        local_df: pd.DataFrame,
        remote_result_df: pd.DataFrame,
        formula_columns: Optional[set],
    ) -> Dict[str, Any]:
        """
        æ£€æµ‹æœ¬åœ°æ•°æ®ä¸äº‘ç«¯ç»“æœçš„å·®å¼‚ï¼Œç”Ÿæˆåˆ—çº§å·®å¼‚æŠ¥å‘Š

        Args:
            local_df: æœ¬åœ°æ•°æ®
            remote_result_df: äº‘ç«¯ç»“æœæ•°æ®
            formula_columns: å…¬å¼åˆ—é›†åˆ

        Returns:
            å·®å¼‚ç»Ÿè®¡å­—å…¸
        """
        if formula_columns is None:
            formula_columns = set()

        diff_stats = {
            "formula_columns": {},  # å…¬å¼åˆ—å·®å¼‚: {åˆ—å: å·®å¼‚è¡Œæ•°}
            "data_columns": {},  # æ•°æ®åˆ—å·®å¼‚: {åˆ—å: å·®å¼‚è¡Œæ•°}
            "error_columns": {},  # å¼‚å¸¸åˆ—: {åˆ—å: é”™è¯¯ä¿¡æ¯}
            "total_rows": len(local_df),
        }

        # éå†æ‰€æœ‰åˆ—
        for col in local_df.columns:
            if col not in remote_result_df.columns:
                diff_stats["error_columns"][col] = "äº‘ç«¯ä¸å­˜åœ¨æ­¤åˆ—"
                continue

            try:
                diff_count = 0
                local_col = local_df[col]
                remote_col = remote_result_df[col]

                # é€è¡Œæ¯”è¾ƒ
                for idx in range(len(local_col)):
                    if idx >= len(remote_col):
                        diff_count += 1
                        continue

                    local_val = local_col.iloc[idx]
                    remote_val = remote_col.iloc[idx]

                    if not self._values_equal(local_val, remote_val):
                        diff_count += 1

                # è®°å½•å·®å¼‚
                if diff_count > 0:
                    if col in formula_columns:
                        diff_stats["formula_columns"][col] = diff_count
                    else:
                        diff_stats["data_columns"][col] = diff_count

            except Exception as e:
                diff_stats["error_columns"][col] = str(e)

        return diff_stats

    def _values_equal(self, val1: Any, val2: Any) -> bool:
        """
        æ¯”è¾ƒä¸¤ä¸ªå€¼æ˜¯å¦ç›¸ç­‰ï¼ˆè€ƒè™‘æ•°å€¼å®¹å·®ï¼‰

        Args:
            val1: ç¬¬ä¸€ä¸ªå€¼
            val2: ç¬¬äºŒä¸ªå€¼

        Returns:
            æ˜¯å¦ç›¸ç­‰
        """
        import pandas as pd
        import numpy as np

        # éƒ½æ˜¯ç©ºå€¼
        if pd.isnull(val1) and pd.isnull(val2):
            return True

        # ä¸€ä¸ªç©ºä¸€ä¸ªä¸ç©º
        if pd.isnull(val1) or pd.isnull(val2):
            return False

        # éƒ½æ˜¯æ•°å€¼ç±»å‹
        try:
            num1 = float(val1)
            num2 = float(val2)
            return abs(num1 - num2) <= self.config.sheet_diff_tolerance
        except (ValueError, TypeError):
            pass

        # å­—ç¬¦ä¸²æ¯”è¾ƒ
        return str(val1).strip() == str(val2).strip()

    def print_column_diff_report(self, diff_stats: Dict[str, Any]):
        """
        æ‰“å°åˆ—çº§å·®å¼‚æŠ¥å‘Š

        Args:
            diff_stats: å·®å¼‚ç»Ÿè®¡å­—å…¸
        """
        if not self.config.sheet_report_column_diff:
            return

        total_rows = diff_stats["total_rows"]
        formula_cols = diff_stats["formula_columns"]
        data_cols = diff_stats["data_columns"]
        error_cols = diff_stats["error_columns"]

        # ç»Ÿè®¡ä¿¡æ¯
        total_cols = len(formula_cols) + len(data_cols) + len(error_cols)
        diff_cols = len(formula_cols) + len(data_cols)

        print("\n" + "=" * 60)
        print("ğŸ“Š åˆ—å·®å¼‚æ£€æµ‹æŠ¥å‘Š")
        print(f"æ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æ¨¡å¼: é€»è¾‘åŒæ­¥+ç»“æœæ£€æµ‹")
        print("=" * 60)

        if formula_cols:
            print("\nğŸ”’ å…¬å¼åˆ—ï¼ˆå·²ä¿æŠ¤ï¼Œä¸è¦†ç›–ï¼‰:")
            for col, diff_count in sorted(formula_cols.items()):
                pct = (diff_count / total_rows * 100) if total_rows > 0 else 0
                print(f"  âœ“ {col}: {diff_count}/{total_rows} è¡Œç»“æœä¸ä¸€è‡´ ({pct:.2f}%)")
            if self.config.sheet_protect_formulas:
                print("  â†’ å»ºè®®: æ£€æŸ¥è¾“å…¥æ•°æ®åˆ—æ˜¯å¦å˜åŒ–")

        if data_cols:
            print("\nğŸ“ æ•°æ®åˆ—ï¼ˆå·²åŒæ­¥ï¼‰:")
            for col, diff_count in sorted(data_cols.items()):
                print(f"  âœ“ {col}: {diff_count} è¡Œå·®å¼‚ â†’ å·²æ›´æ–°")

        if error_cols:
            print("\nâš ï¸  å¼‚å¸¸åˆ—ï¼ˆç±»å‹ä¸åŒ¹é…æˆ–æ— æ³•æ¯”è¾ƒï¼‰:")
            for col, error in sorted(error_cols.items()):
                print(f"  âœ— {col}: {error}")

        print("\n" + "=" * 60)
        print(f"æ€»è®¡: {diff_cols}/{total_cols} åˆ—æœ‰å·®å¼‚")
        if self.config.sheet_protect_formulas:
            print(f"åŒæ­¥å®Œæˆ: {len(data_cols)}/{total_cols} åˆ—")
            print(f"ä¿æŠ¤è·³è¿‡: {len(formula_cols)}/{total_cols} åˆ—")
        else:
            print(f"åŒæ­¥å®Œæˆ: {len(data_cols) + len(formula_cols)}/{total_cols} åˆ—")
        print("=" * 60 + "\n")

    # ========== é€‰æ‹©æ€§åŒæ­¥è¾…åŠ©æ–¹æ³• ==========

    def _get_effective_selective_columns(self, df: pd.DataFrame) -> List[str]:
        """è·å–é€‰æ‹©æ€§åŒæ­¥å®é™…ç”Ÿæ•ˆçš„åˆ—ï¼ˆå«ç´¢å¼•åˆ—ï¼‰"""
        if (
            not self.config.selective_sync.enabled
            or not self.config.selective_sync.columns
        ):
            return df.columns.tolist()

        target_columns = self.config.selective_sync.columns.copy()

        # è‡ªåŠ¨åŒ…å«ç´¢å¼•åˆ—ï¼ˆç”¨äºåŒ¹é…é€»è¾‘ï¼‰
        if (
            self.config.selective_sync.auto_include_index
            and self.config.index_column
            and self.config.index_column not in target_columns
        ):
            target_columns.append(self.config.index_column)
            self.logger.info(f"è‡ªåŠ¨åŒ…å«ç´¢å¼•åˆ—: {self.config.index_column}")

        # å»é‡ï¼Œä¿ç•™é¡ºåº
        deduped_columns = []
        seen = set()
        for col in target_columns:
            if col not in seen:
                seen.add(col)
                deduped_columns.append(col)

        # éªŒè¯åˆ—æ˜¯å¦å­˜åœ¨
        missing_columns = [col for col in deduped_columns if col not in df.columns]
        if missing_columns:
            self.logger.warning(f"æŒ‡å®šçš„åˆ—ä¸å­˜åœ¨äºæ•°æ®ä¸­: {missing_columns}")
            deduped_columns = [col for col in deduped_columns if col in df.columns]

        # ä¿æŒåˆ—é¡ºåºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.selective_sync.preserve_column_order:
            return [col for col in df.columns if col in deduped_columns]

        return deduped_columns

    def _apply_selective_filter(self, df: pd.DataFrame) -> pd.DataFrame:
        """åº”ç”¨é€‰æ‹©æ€§åˆ—è¿‡æ»¤"""
        if (
            not self.config.selective_sync.enabled
            or not self.config.selective_sync.columns
        ):
            return df

        # è·å–è¦å¤„ç†çš„åˆ—
        target_columns = self._get_effective_selective_columns(df)
        return df[target_columns]

    # ========== Bitable å­—æ®µæŸ¥è¯¢ä¼˜åŒ– ==========

    def _get_bitable_fetch_field_names(
        self, df: pd.DataFrame, mode: str
    ) -> Optional[List[str]]:
        """
        æ ¹æ®åŒæ­¥æ¨¡å¼è®¡ç®—è·å–è¿œç¨‹è®°å½•æ—¶éœ€è¦çš„å­—æ®µåˆ—è¡¨ã€‚

        é€šè¿‡é£ä¹¦æŸ¥è¯¢è®°å½•APIçš„ field_names å‚æ•°ï¼Œåªè¿”å›å¿…è¦çš„å­—æ®µï¼Œ
        å‡å°‘ä¸å¿…è¦çš„æ•°æ®ä¼ è¾“ï¼Œæå‡æŸ¥è¯¢æ€§èƒ½ã€‚

        Args:
            df: æœ¬åœ°æ•°æ® DataFrame
            mode: åŒæ­¥æ¨¡å¼ ('full', 'incremental', 'overwrite', 'clone')

        Returns:
            field_names åˆ—è¡¨ï¼ŒNone è¡¨ç¤ºè·å–å…¨éƒ¨å­—æ®µ
        """
        if mode == "clone":
            # clone æ¨¡å¼åªéœ€ record_idï¼ˆAPI å›ºå®šè¿”å›ï¼‰ï¼Œä½¿ç”¨ç©º field_names è¿”å›æœ€å°å­—æ®µé›†
            return []

        index_col = self.config.index_column
        if not index_col:
            return None  # æ— ç´¢å¼•åˆ—æ—¶æ— æ³•ä¼˜åŒ–

        # full / incremental / overwriteï¼šä»…éœ€ç´¢å¼•åˆ—ç”¨äºåŒ¹é…å’Œè·å– record_id
        if mode in ("full", "incremental", "overwrite"):
            return [index_col]

        return None

    # ========== ç»Ÿä¸€åŒæ­¥æ–¹æ³• ==========

    def sync_full(self, df: pd.DataFrame) -> bool:
        """å…¨é‡åŒæ­¥ï¼šå·²å­˜åœ¨çš„æ›´æ–°ï¼Œä¸å­˜åœ¨çš„æ–°å¢"""
        self.logger.info("å¼€å§‹å…¨é‡åŒæ­¥...")

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨é€‰æ‹©æ€§åŒæ­¥
        if self.config.selective_sync.enabled:
            df = self._apply_selective_filter(df)
            self.logger.info(
                f"é€‰æ‹©æ€§åŒæ­¥å·²å¯ç”¨ï¼Œå¤„ç† {len(self.config.selective_sync.columns) if self.config.selective_sync.columns else 'æ‰€æœ‰'} åˆ—"
            )

        if self.config.target_type == TargetType.BITABLE:
            return self._sync_full_bitable(df)
        else:  # SHEET
            return self._sync_full_sheet(df)

    def _sync_full_bitable(self, df: pd.DataFrame) -> bool:
        """å¤šç»´è¡¨æ ¼å…¨é‡åŒæ­¥"""
        if not self.config.index_column:
            self.logger.warning("æœªæŒ‡å®šç´¢å¼•åˆ—ï¼Œå°†æ‰§è¡Œçº¯æ–°å¢æ“ä½œ")
            field_types = self.get_field_types()
            new_records = self.converter.df_to_records(df, field_types)
            if (
                isinstance(self.api, BitableAPI)
                and self.config.app_token
                and self.config.table_id
            ):
                return self.process_in_batches(
                    new_records,
                    self.config.batch_size,
                    self.api.batch_create_records,
                    self.config.app_token,
                    self.config.table_id,
                )
            return False

        # è·å–ç°æœ‰è®°å½•å¹¶å»ºç«‹ç´¢å¼•ï¼ˆä½¿ç”¨field_namesä¼˜åŒ–ï¼Œå‡å°‘æ•°æ®ä¼ è¾“ï¼‰
        fetch_fields = self._get_bitable_fetch_field_names(df, "full")
        existing_records = self.get_all_bitable_records(field_names=fetch_fields)
        self.logger.info(f"ğŸ” è·å–åˆ°ç°æœ‰è®°å½•æ•°é‡: {len(existing_records)}")

        existing_index = self.converter.build_record_index(
            existing_records, self.config.index_column
        )
        self.logger.info(f"ğŸ” æ„å»ºç´¢å¼•æˆåŠŸï¼Œç´¢å¼•æ•°é‡: {len(existing_index)}")

        # æ‰“å°å‰å‡ ä¸ªç°æœ‰è®°å½•çš„ç´¢å¼•åˆ—å€¼ç”¨äºè°ƒè¯•
        if existing_records and len(existing_records) > 0:
            for i, record in enumerate(existing_records[:3]):
                fields = record.get("fields", {})
                index_value = fields.get(self.config.index_column, "æœªæ‰¾åˆ°")
                self.logger.info(
                    f"ğŸ” ç°æœ‰è®°å½• {i+1} ç´¢å¼•åˆ— '{self.config.index_column}' å€¼: '{index_value}'"
                )

        field_types = self.get_field_types()

        # åˆ†ç±»æœ¬åœ°æ•°æ®
        records_to_update = []
        records_to_create = []

        for i, (_, row) in enumerate(df.iterrows()):
            index_hash = self.converter.get_index_value_hash(
                row, self.config.index_column
            )
            index_value = row.get(self.config.index_column, "æœªæ‰¾åˆ°")

            # æ‰“å°å‰å‡ æ¡è®°å½•çš„åŒ¹é…ä¿¡æ¯ç”¨äºè°ƒè¯•
            if i < 3:
                self.logger.info(
                    f"ğŸ” æ–°æ•°æ®è®°å½• {i+1} ç´¢å¼•åˆ— '{self.config.index_column}' å€¼: '{index_value}' -> å“ˆå¸Œ: {index_hash}"
                )
                self.logger.info(
                    f"ğŸ” å“ˆå¸Œæ˜¯å¦åœ¨ç°æœ‰ç´¢å¼•ä¸­: {index_hash in existing_index if index_hash else False}"
                )

            # ä½¿ç”¨å­—æ®µç±»å‹è½¬æ¢æ„å»ºè®°å½•
            fields = {}
            for k, v in row.to_dict().items():
                if pd.notnull(v):
                    converted_value = self.converter.convert_field_value_safe(
                        str(k), v, field_types
                    )
                    if converted_value is not None:
                        fields[str(k)] = converted_value

            record = {"fields": fields}

            if index_hash and index_hash in existing_index:
                # éœ€è¦æ›´æ–°çš„è®°å½•
                existing_record = existing_index[index_hash]
                record["record_id"] = existing_record["record_id"]
                records_to_update.append(record)
            else:
                # éœ€è¦æ–°å¢çš„è®°å½•
                records_to_create.append(record)

        self.logger.info(
            f"å…¨é‡åŒæ­¥è®¡åˆ’: æ›´æ–° {len(records_to_update)} æ¡ï¼Œæ–°å¢ {len(records_to_create)} æ¡"
        )

        # æ‰§è¡Œæ›´æ–°
        update_success = True
        if (
            records_to_update
            and isinstance(self.api, BitableAPI)
            and self.config.app_token
            and self.config.table_id
        ):
            update_success = self.process_in_batches(
                records_to_update,
                self.config.batch_size,
                self.api.batch_update_records,
                self.config.app_token,
                self.config.table_id,
            )

        # æ‰§è¡Œæ–°å¢
        create_success = True
        if (
            records_to_create
            and isinstance(self.api, BitableAPI)
            and self.config.app_token
            and self.config.table_id
        ):
            create_success = self.process_in_batches(
                records_to_create,
                self.config.batch_size,
                self.api.batch_create_records,
                self.config.app_token,
                self.config.table_id,
            )

        return update_success and create_success

    def _sync_full_sheet(self, df: pd.DataFrame) -> bool:
        """ç”µå­è¡¨æ ¼å…¨é‡åŒæ­¥"""
        if not self.config.index_column:
            self.logger.warning("æœªæŒ‡å®šç´¢å¼•åˆ—ï¼Œå°†æ‰§è¡Œå®Œå…¨è¦†ç›–æ“ä½œ")
            return self.sync_clone(df)

        # è·å–ç°æœ‰æ•°æ®ï¼ˆæ”¯æŒåŒè¯»å’Œå·®å¼‚æ£€æµ‹ï¼‰
        current_df, formula_df, formula_columns = self.get_sheet_data_with_validation()

        if current_df.empty:
            self.logger.info("ç”µå­è¡¨æ ¼ä¸ºç©ºï¼Œæ‰§è¡Œæ–°å¢æ“ä½œ")
            return self.sync_clone(df)

        # â­ å…³é”®ä¿®æ”¹ï¼šæ£€æŸ¥æ˜¯å¦å¯ç”¨é€‰æ‹©æ€§åŒæ­¥ï¼Œä½¿ç”¨ç²¾ç¡®åˆ—çº§æ§åˆ¶
        if self.config.selective_sync.enabled and self.config.selective_sync.columns:
            self.logger.info(
                f"ğŸ¯ å¯ç”¨ç²¾ç¡®åˆ—çº§æ§åˆ¶åŒæ­¥: {self.config.selective_sync.columns}"
            )
            return self._sync_selective_columns_sheet(df, current_df)

        # å·®å¼‚æ£€æµ‹ä¸æŠ¥å‘Š
        if self.config.sheet_validate_results and formula_columns is not None:
            diff_stats = self.validate_and_report_differences(
                df, current_df, formula_columns
            )
            self.print_column_diff_report(diff_stats)

        # å…¬å¼ä¿æŠ¤ï¼šè¿‡æ»¤æ‰å…¬å¼åˆ—
        sync_df = df
        if self.config.sheet_protect_formulas and formula_columns:
            # åªåŒæ­¥éå…¬å¼åˆ—
            non_formula_cols = [col for col in df.columns if col not in formula_columns]
            if not non_formula_cols:
                self.logger.warning("æ‰€æœ‰åˆ—éƒ½æ˜¯å…¬å¼åˆ—ï¼Œä¸”å¯ç”¨äº†å…¬å¼ä¿æŠ¤ï¼Œæ— éœ€åŒæ­¥")
                return True
            sync_df = df[non_formula_cols].copy()
            self.logger.info(f"ğŸ”’ å…¬å¼ä¿æŠ¤å·²å¯ç”¨ï¼Œä»…åŒæ­¥ {len(non_formula_cols)} ä¸ªæ•°æ®åˆ—")

        # åŸæœ‰çš„å®Œæ•´è¡¨æ ¼åŒæ­¥é€»è¾‘
        current_index = self.converter.build_data_index(
            current_df, self.config.index_column
        )

        # åˆ†ç±»æ•°æ®
        update_rows = []
        new_rows = []

        for _, row in sync_df.iterrows():
            index_hash = self.converter.get_index_value_hash(
                row, self.config.index_column
            )
            if index_hash and index_hash in current_index:
                # æ›´æ–°ç°æœ‰è¡Œ
                current_row_idx = current_index[index_hash]
                update_rows.append((current_row_idx, row))
            else:
                # æ–°å¢è¡Œ
                new_rows.append(row)

        self.logger.info(
            f"å…¨é‡åŒæ­¥è®¡åˆ’: æ›´æ–° {len(update_rows)} è¡Œï¼Œæ–°å¢ {len(new_rows)} è¡Œ"
        )

        # æ‰§è¡Œæ›´æ–°
        success = True
        if update_rows:
            # æ›´æ–°ç°æœ‰è¡Œ
            updated_df = current_df.copy()
            for current_row_idx, new_row in update_rows:
                for col in sync_df.columns:
                    if col in updated_df.columns:
                        # ä½¿ç”¨ .iloc åŒç´¢å¼•é¿å…é“¾å¼èµ‹å€¼é—®é¢˜ (SettingWithCopyWarning)
                        updated_df.iloc[current_row_idx, updated_df.columns.get_loc(col)] = new_row[col]

            # å†™å…¥æ›´æ–°åçš„æ•°æ®
            values = self.converter.df_to_values(updated_df)
            if (
                isinstance(self.api, SheetAPI)
                and self.config.spreadsheet_token
                and self.config.sheet_id
            ):
                success = self.api.write_sheet_data(
                    self.config.spreadsheet_token,
                    self.config.sheet_id,
                    values,
                    self.config.batch_size,
                    80,  # åˆ—æ‰¹æ¬¡å¤§å°ï¼Œä¿æŒå®‰å…¨è£•åº¦
                    self.config.rate_limit_delay,
                )

        # è¿½åŠ æ–°è¡Œ
        if new_rows and success:
            new_df = pd.DataFrame(new_rows)
            new_values = self.converter.df_to_values(new_df, include_headers=False)

            if (
                new_values
                and isinstance(self.api, SheetAPI)
                and self.config.spreadsheet_token
                and self.config.sheet_id
            ):
                self.logger.info(f"å¼€å§‹è¿½åŠ  {len(new_values)} è¡Œæ–°æ•°æ®")
                success = self.api.append_sheet_data(
                    self.config.spreadsheet_token,
                    self.config.sheet_id,
                    new_values,
                    self.config.batch_size,
                    self.config.rate_limit_delay,
                )

        return success

    def _sync_selective_columns_sheet(
        self, df: pd.DataFrame, current_df: pd.DataFrame
    ) -> bool:
        """ç”µå­è¡¨æ ¼é€‰æ‹©æ€§åˆ—åŒæ­¥ - ä½¿ç”¨ç²¾ç¡®åˆ—æ§åˆ¶"""
        columns = self._get_effective_selective_columns(df)
        if not columns:
            self.logger.warning("é€‰æ‹©æ€§åˆ—åŒæ­¥æœªé…ç½® columns æˆ–æ— å¯ç”¨åˆ—ï¼Œå·²è·³è¿‡")
            return False
        self.logger.info(f"ğŸ¯ å¯ç”¨ç²¾ç¡®åˆ—æ§åˆ¶åŒæ­¥: {columns}")

        # æ„å»ºç´¢å¼•
        current_index = self.converter.build_data_index(
            current_df, self.config.index_column
        )

        # å‡†å¤‡æ›´æ–°æ•°æ®æ˜ å°„ {row_idx: {col: value}}
        update_data_map: Dict[int, Dict[str, Any]] = {}
        new_rows: List[pd.Series] = []

        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(
                row, self.config.index_column
            )
            if index_hash and index_hash in current_index:
                # æ›´æ–°ç°æœ‰è¡Œ
                current_row_idx = current_index[index_hash]
                if current_row_idx not in update_data_map:
                    update_data_map[current_row_idx] = {}

                # åªæ›´æ–°æŒ‡å®šåˆ—
                for col in columns:
                    if col in df.columns:
                        update_data_map[current_row_idx][col] = row[col]
            else:
                # æ–°å¢è¡Œ
                new_rows.append(row)

        self.logger.info(
            f"ç²¾ç¡®åˆ—æ§åˆ¶è®¡åˆ’: æ›´æ–° {len(update_data_map)} è¡Œçš„æŒ‡å®šåˆ—ï¼Œæ–°å¢ {len(new_rows)} è¡Œ"
        )

        success = True

        # æ‰§è¡Œé€‰æ‹©æ€§åˆ—æ›´æ–°
        if update_data_map:
            success = self._update_selective_columns(current_df, update_data_map)

        # è¿½åŠ æ–°è¡Œï¼ˆå¦‚æœæœ‰ï¼‰
        if new_rows and success:
            new_df = pd.DataFrame(new_rows)
            new_values = self.converter.df_to_values(new_df, include_headers=False)

            if (
                isinstance(self.api, SheetAPI)
                and self.config.spreadsheet_token
                and self.config.sheet_id
            ):
                self.logger.info(f"å¼€å§‹è¿½åŠ  {len(new_values)} è¡Œæ–°æ•°æ®")
                success = self.api.append_sheet_data(
                    self.config.spreadsheet_token,
                    self.config.sheet_id,
                    new_values,
                    self.config.batch_size,
                    self.config.rate_limit_delay,
                )

        return success

    def _update_selective_columns(
        self, current_df: pd.DataFrame, update_data_map: Dict[int, Dict[str, Any]]
    ) -> bool:
        """ä½¿ç”¨ç²¾ç¡®åˆ—æ§åˆ¶æ›´æ–°æ•°æ®"""
        if not update_data_map:
            return True

        # å‡†å¤‡æŒ‰åˆ—ç»„ç»‡çš„æ›´æ–°æ•°æ®
        columns_to_update: set[str] = set()
        for row_updates in update_data_map.values():
            columns_to_update.update(row_updates.keys())

        self.logger.info(f"ğŸ”„ å‡†å¤‡æ›´æ–°åˆ—: {list(columns_to_update)}")

        # ä½¿ç”¨ converter çš„ df_to_column_data å’Œ get_column_positions
        # æ„å»ºéœ€è¦æ›´æ–°çš„åˆ—æ•°æ®
        column_data = {}
        for col in columns_to_update:
            col_data = []
            for row_idx in range(len(current_df)):
                if row_idx in update_data_map and col in update_data_map[row_idx]:
                    # ä½¿ç”¨æ–°å€¼
                    converted_value = self.converter.simple_convert_value(
                        update_data_map[row_idx][col]
                    )
                    col_data.append(converted_value)
                else:
                    # ä¿æŒåŸå€¼
                    original_value = (
                        current_df.iloc[row_idx][col]
                        if col in current_df.columns
                        else ""
                    )
                    converted_value = self.converter.simple_convert_value(
                        original_value
                    )
                    col_data.append(converted_value)
            column_data[col] = col_data

        # è·å–èµ·å§‹åˆ—åç§»é‡
        start_col_offset = 0
        if isinstance(self.api, SheetAPI):
            start_col_offset = self.api.start_col_num - 1

        # è·å–åˆ—ä½ç½®æ˜ å°„ï¼ˆè€ƒè™‘èµ·å§‹åˆ—åç§»ï¼‰
        column_positions = self.converter.get_column_positions(
            current_df, list(columns_to_update), start_col_offset
        )

        self.logger.info(f"ğŸ“ åˆ—ä½ç½®æ˜ å°„: {column_positions}")

        # ä½¿ç”¨ç²¾ç¡®åˆ—å†™å…¥API
        if (
            isinstance(self.api, SheetAPI)
            and self.config.spreadsheet_token
            and self.config.sheet_id
        ):
            # start_row éœ€è¦è€ƒè™‘é…ç½®çš„èµ·å§‹è¡Œ + è¡¨å¤´è¡Œ
            actual_start_row = self.config.start_row + 1
            # å¦‚æœ optimize_ranges ä¸º Falseï¼Œè®¾ç½® max_gap=0 ç¦ç”¨åˆå¹¶
            effective_max_gap = (
                self.config.selective_sync.max_gap_for_merge
                if self.config.selective_sync.optimize_ranges
                else 0
            )
            return self.api.write_selective_columns(
                self.config.spreadsheet_token,
                self.config.sheet_id,
                column_data,
                column_positions,
                start_row=actual_start_row,
                rate_limit_delay=self.config.rate_limit_delay,
                max_gap=effective_max_gap,
            )

        return False

    def sync_incremental(self, df: pd.DataFrame) -> bool:
        """å¢é‡åŒæ­¥ï¼šåªæ–°å¢ä¸å­˜åœ¨çš„è®°å½•"""
        self.logger.info("å¼€å§‹å¢é‡åŒæ­¥...")

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨é€‰æ‹©æ€§åŒæ­¥
        if self.config.selective_sync.enabled:
            df = self._apply_selective_filter(df)
            self.logger.info(
                f"é€‰æ‹©æ€§åŒæ­¥å·²å¯ç”¨ï¼Œå¤„ç† {len(self.config.selective_sync.columns) if self.config.selective_sync.columns else 'æ‰€æœ‰'} åˆ—"
            )

        if self.config.target_type == TargetType.BITABLE:
            return self._sync_incremental_bitable(df)
        else:  # SHEET
            return self._sync_incremental_sheet(df)

    def _sync_incremental_bitable(self, df: pd.DataFrame) -> bool:
        """å¤šç»´è¡¨æ ¼å¢é‡åŒæ­¥"""
        if not self.config.index_column:
            self.logger.warning("æœªæŒ‡å®šç´¢å¼•åˆ—ï¼Œå°†æ‰§è¡Œçº¯æ–°å¢æ“ä½œ")
            field_types = self.get_field_types()
            new_records = self.converter.df_to_records(df, field_types)
            if (
                isinstance(self.api, BitableAPI)
                and self.config.app_token
                and self.config.table_id
            ):
                return self.process_in_batches(
                    new_records,
                    self.config.batch_size,
                    self.api.batch_create_records,
                    self.config.app_token,
                    self.config.table_id,
                )
            return False

        # è·å–ç°æœ‰è®°å½•å¹¶å»ºç«‹ç´¢å¼•ï¼ˆä»…è·å–ç´¢å¼•åˆ—ï¼Œå‡å°‘æ•°æ®ä¼ è¾“ï¼‰
        fetch_fields = self._get_bitable_fetch_field_names(df, "incremental")
        existing_records = self.get_all_bitable_records(field_names=fetch_fields)
        existing_index = self.converter.build_record_index(
            existing_records, self.config.index_column
        )
        field_types = self.get_field_types()

        # ç­›é€‰å‡ºéœ€è¦æ–°å¢çš„è®°å½•
        records_to_create = []

        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(
                row, self.config.index_column
            )

            if not index_hash or index_hash not in existing_index:
                # ä½¿ç”¨å­—æ®µç±»å‹è½¬æ¢æ„å»ºè®°å½•
                fields = {}
                for k, v in row.to_dict().items():
                    if pd.notnull(v):
                        converted_value = self.converter.convert_field_value_safe(
                            str(k), v, field_types
                        )
                        if converted_value is not None:
                            fields[str(k)] = converted_value

                record = {"fields": fields}
                records_to_create.append(record)

        self.logger.info(f"å¢é‡åŒæ­¥è®¡åˆ’: æ–°å¢ {len(records_to_create)} æ¡è®°å½•")

        if (
            records_to_create
            and isinstance(self.api, BitableAPI)
            and self.config.app_token
            and self.config.table_id
        ):
            return self.process_in_batches(
                records_to_create,
                self.config.batch_size,
                self.api.batch_create_records,
                self.config.app_token,
                self.config.table_id,
            )
        else:
            self.logger.info("æ²¡æœ‰æ–°è®°å½•éœ€è¦åŒæ­¥")
            return True

    def _sync_incremental_sheet(self, df: pd.DataFrame) -> bool:
        """ç”µå­è¡¨æ ¼å¢é‡åŒæ­¥ - ä½¿ç”¨ä¼˜åŒ–APIç­–ç•¥"""
        if not self.config.index_column:
            self.logger.warning("æœªæŒ‡å®šç´¢å¼•åˆ—ï¼Œå°†æ–°å¢å…¨éƒ¨æ•°æ®")

            # â­ æ£€æŸ¥é€‰æ‹©æ€§åŒæ­¥ï¼šå¦‚æœå¯ç”¨ï¼Œéœ€è¦ç”¨åˆ—çº§æ§åˆ¶è¿½åŠ 
            if (
                self.config.selective_sync.enabled
                and self.config.selective_sync.columns
            ):
                self.logger.info(
                    f"ğŸ¯ å¢é‡åŒæ­¥å¯ç”¨ç²¾ç¡®åˆ—æ§åˆ¶: {self.config.selective_sync.columns}"
                )
                return self._append_selective_columns(df)

            # å¸¸è§„å¢é‡åŒæ­¥ç­–ç•¥
            values = self.converter.df_to_values(
                df, include_headers=False
            )  # è¿½åŠ ä¸éœ€è¦è¡¨å¤´
            self.logger.info("ä½¿ç”¨appendæ¥å£è¿›è¡Œå¢é‡åŒæ­¥")
            if (
                isinstance(self.api, SheetAPI)
                and self.config.spreadsheet_token
                and self.config.sheet_id
            ):
                return self.api.append_sheet_data(
                    self.config.spreadsheet_token,
                    self.config.sheet_id,
                    values,
                    self.config.batch_size,
                    self.config.rate_limit_delay,
                )
            return False

        # è·å–ç°æœ‰æ•°æ®
        current_df = self.get_current_sheet_data()

        if current_df.empty:
            self.logger.info("ç”µå­è¡¨æ ¼ä¸ºç©ºï¼Œæ–°å¢å…¨éƒ¨æ•°æ®")
            # â­ æ£€æŸ¥é€‰æ‹©æ€§åŒæ­¥
            if (
                self.config.selective_sync.enabled
                and self.config.selective_sync.columns
            ):
                return self._append_selective_columns(df)
            # ä½¿ç”¨å…‹éš†åŒæ­¥ï¼ˆä¼šå…ˆå†™å…¥æ•°æ®å†è®¾ç½®æ ¼å¼ï¼‰
            return self.sync_clone(df)

        # æ„å»ºç´¢å¼•
        current_index = self.converter.build_data_index(
            current_df, self.config.index_column
        )

        # ç­›é€‰éœ€è¦æ–°å¢çš„è®°å½•
        new_rows = []
        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(
                row, self.config.index_column
            )
            if not index_hash or index_hash not in current_index:
                new_rows.append(row)

        self.logger.info(f"å¢é‡åŒæ­¥è®¡åˆ’: æ–°å¢ {len(new_rows)} è¡Œ")

        if new_rows:
            new_df = pd.DataFrame(new_rows)

            # â­ æ£€æŸ¥é€‰æ‹©æ€§åŒæ­¥ï¼šå¦‚æœå¯ç”¨ï¼Œéœ€è¦ç”¨åˆ—çº§æ§åˆ¶è¿½åŠ 
            if (
                self.config.selective_sync.enabled
                and self.config.selective_sync.columns
            ):
                return self._append_selective_columns(new_df)

            # å¸¸è§„è¿½åŠ 
            new_values = self.converter.df_to_values(new_df, include_headers=False)

            # è¿½åŠ æ–°æ•°æ®
            if (
                isinstance(self.api, SheetAPI)
                and self.config.spreadsheet_token
                and self.config.sheet_id
            ):
                self.logger.info(f"å¼€å§‹å¢é‡è¿½åŠ  {len(new_values)} è¡Œæ•°æ®")
                return self.api.append_sheet_data(
                    self.config.spreadsheet_token,
                    self.config.sheet_id,
                    new_values,
                    self.config.batch_size,
                    self.config.rate_limit_delay,
                )
            return False
        else:
            self.logger.info("æ²¡æœ‰æ–°è®°å½•éœ€è¦åŒæ­¥")
            return True

    def _append_selective_columns(self, df: pd.DataFrame) -> bool:
        """é€‰æ‹©æ€§åˆ—çš„è¿½åŠ æ“ä½œ"""
        if (
            not self.config.selective_sync.enabled
            or not self.config.selective_sync.columns
        ):
            self.logger.warning("é€‰æ‹©æ€§åŒæ­¥æœªå¯ç”¨æˆ–æœªæŒ‡å®šåˆ—ï¼Œä½¿ç”¨å¸¸è§„è¿½åŠ ")
            values = self.converter.df_to_values(df, include_headers=False)
            if (
                isinstance(self.api, SheetAPI)
                and self.config.spreadsheet_token
                and self.config.sheet_id
            ):
                return self.api.append_sheet_data(
                    self.config.spreadsheet_token,
                    self.config.sheet_id,
                    values,
                    self.config.batch_size,
                    self.config.rate_limit_delay,
                )
            return False

        # è·å–å½“å‰è¡¨æ ¼æ•°æ®ä»¥ç¡®å®šæ­£ç¡®çš„åˆ—ä½ç½®
        current_df = self.get_current_sheet_data()

        effective_columns = self._get_effective_selective_columns(df)
        if not effective_columns:
            self.logger.warning("é€‰æ‹©æ€§åˆ—è¿½åŠ æ— å¯ç”¨åˆ—ï¼Œå·²è·³è¿‡")
            return False

        if current_df.empty:
            # å¦‚æœè¡¨æ ¼ä¸ºç©ºï¼Œå…ˆå†™å…¥è¡¨å¤´ï¼Œç„¶åè¿½åŠ æ•°æ®
            self.logger.info("è¡¨æ ¼ä¸ºç©ºï¼Œå…ˆåˆ›å»ºè¡¨å¤´ç„¶åè¿½åŠ é€‰æ‹©æ€§åˆ—æ•°æ®")
            header_values = [effective_columns]

            # å†™å…¥è¡¨å¤´
            if (
                isinstance(self.api, SheetAPI)
                and self.config.spreadsheet_token
                and self.config.sheet_id
            ):
                header_success = self.api.write_sheet_data(
                    self.config.spreadsheet_token,
                    self.config.sheet_id,
                    header_values,
                    self.config.batch_size,
                    80,
                    self.config.rate_limit_delay,
                )
                if not header_success:
                    return False

                # æ›´æ–°current_dfä¸ºåŒ…å«è¡¨å¤´çš„ç©ºæ•°æ®æ¡†
                current_df = pd.DataFrame(columns=effective_columns)

        # å‡†å¤‡é€‰æ‹©æ€§åˆ—æ•°æ®
        column_data = self.converter.df_to_column_data(
            df, effective_columns
        )
        
        # è·å–èµ·å§‹åˆ—åç§»é‡
        start_col_offset = 0
        if isinstance(self.api, SheetAPI):
            start_col_offset = self.api.start_col_num - 1
            
        column_positions = self.converter.get_column_positions(
            current_df, effective_columns, start_col_offset
        )

        # è®¡ç®—èµ·å§‹è¡Œï¼šé…ç½®çš„èµ·å§‹è¡Œ + å½“å‰æ•°æ®è¡Œæ•° + 1ï¼ˆè¡¨å¤´ï¼‰
        start_row = self.config.start_row + len(current_df) + 1

        self.logger.info(
            f"ğŸ¯ é€‰æ‹©æ€§åˆ—è¿½åŠ : {list(column_data.keys())} ä»ç¬¬{start_row}è¡Œå¼€å§‹"
        )

        # ä½¿ç”¨ç²¾ç¡®åˆ—è¿½åŠ API
        if (
            isinstance(self.api, SheetAPI)
            and self.config.spreadsheet_token
            and self.config.sheet_id
        ):
            effective_max_gap = (
                self.config.selective_sync.max_gap_for_merge
                if self.config.selective_sync.optimize_ranges
                else 0
            )
            return self.api.write_selective_columns(
                self.config.spreadsheet_token,
                self.config.sheet_id,
                column_data,
                column_positions,
                start_row=start_row,
                rate_limit_delay=self.config.rate_limit_delay,
                max_gap=effective_max_gap,
            )

        return False

    def sync_overwrite(self, df: pd.DataFrame) -> bool:
        """è¦†ç›–åŒæ­¥ï¼šåˆ é™¤å·²å­˜åœ¨çš„ï¼Œç„¶åæ–°å¢å…¨éƒ¨"""
        self.logger.info("å¼€å§‹è¦†ç›–åŒæ­¥...")

        if not self.config.index_column:
            self.logger.error("è¦†ç›–åŒæ­¥æ¨¡å¼éœ€è¦æŒ‡å®šç´¢å¼•åˆ—")
            return False

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨é€‰æ‹©æ€§åŒæ­¥
        if self.config.selective_sync.enabled:
            df = self._apply_selective_filter(df)
            self.logger.info(
                f"é€‰æ‹©æ€§åŒæ­¥å·²å¯ç”¨ï¼Œå¤„ç† {len(self.config.selective_sync.columns) if self.config.selective_sync.columns else 'æ‰€æœ‰'} åˆ—"
            )

        if self.config.target_type == TargetType.BITABLE:
            return self._sync_overwrite_bitable(df)
        else:  # SHEET
            return self._sync_overwrite_sheet(df)

    def _sync_overwrite_bitable(self, df: pd.DataFrame) -> bool:
        """å¤šç»´è¡¨æ ¼è¦†ç›–åŒæ­¥"""
        # è·å–ç°æœ‰è®°å½•å¹¶å»ºç«‹ç´¢å¼•ï¼ˆä»…è·å–ç´¢å¼•åˆ—ï¼Œå‡å°‘æ•°æ®ä¼ è¾“ï¼‰
        fetch_fields = self._get_bitable_fetch_field_names(df, "overwrite")
        existing_records = self.get_all_bitable_records(field_names=fetch_fields)
        existing_index = self.converter.build_record_index(
            existing_records, self.config.index_column
        )
        field_types = self.get_field_types()

        # æ‰¾å‡ºéœ€è¦åˆ é™¤çš„è®°å½•
        record_ids_to_delete = []

        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(
                row, self.config.index_column
            )
            if index_hash and index_hash in existing_index:
                existing_record = existing_index[index_hash]
                record_ids_to_delete.append(existing_record["record_id"])

        self.logger.info(
            f"è¦†ç›–åŒæ­¥è®¡åˆ’: åˆ é™¤ {len(record_ids_to_delete)} æ¡å·²å­˜åœ¨è®°å½•ï¼Œç„¶åæ–°å¢ {len(df)} æ¡è®°å½•"
        )

        # åˆ é™¤å·²å­˜åœ¨çš„è®°å½•
        delete_success = True
        if (
            record_ids_to_delete
            and isinstance(self.api, BitableAPI)
            and self.config.app_token
            and self.config.table_id
        ):
            delete_success = self.process_in_batches(
                record_ids_to_delete,
                self.config.batch_size,
                self.api.batch_delete_records,
                self.config.app_token,
                self.config.table_id,
            )

        # æ–°å¢å…¨éƒ¨è®°å½•
        new_records = self.converter.df_to_records(df, field_types)
        create_success = False
        if (
            isinstance(self.api, BitableAPI)
            and self.config.app_token
            and self.config.table_id
        ):
            create_success = self.process_in_batches(
                new_records,
                self.config.batch_size,
                self.api.batch_create_records,
                self.config.app_token,
                self.config.table_id,
            )

        return delete_success and create_success

    def _sync_overwrite_sheet(self, df: pd.DataFrame) -> bool:
        """ç”µå­è¡¨æ ¼è¦†ç›–åŒæ­¥"""
        # è·å–ç°æœ‰æ•°æ®
        current_df = self.get_current_sheet_data()

        if current_df.empty:
            self.logger.info("ç”µå­è¡¨æ ¼ä¸ºç©ºï¼Œæ‰§è¡Œæ–°å¢æ“ä½œ")
            return self.sync_clone(df)

        # â­ æ£€æŸ¥æ˜¯å¦å¯ç”¨é€‰æ‹©æ€§åŒæ­¥ï¼Œä½¿ç”¨ç²¾ç¡®åˆ—çº§æ§åˆ¶
        if self.config.selective_sync.enabled and self.config.selective_sync.columns:
            self.logger.info(
                f"ğŸ¯ è¦†ç›–åŒæ­¥å¯ç”¨ç²¾ç¡®åˆ—æ§åˆ¶: {self.config.selective_sync.columns}"
            )
            return self._sync_overwrite_selective_columns_sheet(df, current_df)

        # åŸæœ‰çš„å®Œæ•´è¡¨æ ¼è¦†ç›–é€»è¾‘
        new_df_rows = []
        deleted_count = 0

        # ä¿ç•™ä¸åœ¨æ–°æ•°æ®ä¸­çš„ç°æœ‰è®°å½•
        for _, row in current_df.iterrows():
            index_hash = self.converter.get_index_value_hash(
                row, self.config.index_column
            )
            if index_hash:
                # æ£€æŸ¥æ˜¯å¦åœ¨æ–°æ•°æ®ä¸­
                found_in_new = False
                for _, new_row in df.iterrows():
                    new_index_hash = self.converter.get_index_value_hash(
                        new_row, self.config.index_column
                    )
                    if new_index_hash == index_hash:
                        found_in_new = True
                        break

                if not found_in_new:
                    new_df_rows.append(row)
                else:
                    deleted_count += 1

        # æ·»åŠ æ–°æ•°æ®
        for _, row in df.iterrows():
            new_df_rows.append(row)

        self.logger.info(f"è¦†ç›–åŒæ­¥è®¡åˆ’: åˆ é™¤ {deleted_count} è¡Œï¼Œæ–°å¢ {len(df)} è¡Œ")

        # é‡å†™æ•´ä¸ªè¡¨æ ¼
        if new_df_rows:
            new_df = pd.DataFrame(new_df_rows)
            values = self.converter.df_to_values(new_df)

            # ä½¿ç”¨ä¼˜åŒ–APIç­–ç•¥è¦†ç›–å†™å…¥
            if (
                isinstance(self.api, SheetAPI)
                and self.config.spreadsheet_token
                and self.config.sheet_id
            ):
                self.logger.info("ä½¿ç”¨write_sheet_dataè¦†ç›–å†™å…¥")
                return self.api.write_sheet_data(
                    self.config.spreadsheet_token,
                    self.config.sheet_id,
                    values,
                    self.config.batch_size,
                    80,  # col_batch_size
                    self.config.rate_limit_delay,
                )
            return False
        else:
            # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œæ¸…ç©ºè¡¨æ ¼
            if (
                isinstance(self.api, SheetAPI)
                and self.config.spreadsheet_token
                and self.config.sheet_id
            ):
                clear_range = self._build_sheet_full_range()
                if not clear_range:
                    self.logger.error("æ— æ³•è·å–å·¥ä½œè¡¨ç½‘æ ¼èŒƒå›´ï¼Œæ¸…ç©ºå¤±è´¥")
                    return False
                return self.api.clear_sheet_data(
                    self.config.spreadsheet_token, self.config.sheet_id, clear_range
                )
            return False

    def _sync_overwrite_selective_columns_sheet(
        self, df: pd.DataFrame, current_df: pd.DataFrame
    ) -> bool:
        """ç”µå­è¡¨æ ¼é€‰æ‹©æ€§åˆ—è¦†ç›–åŒæ­¥"""
        columns = self._get_effective_selective_columns(df)
        if not columns:
            self.logger.warning("é€‰æ‹©æ€§åˆ—è¦†ç›–åŒæ­¥æœªé…ç½® columns æˆ–æ— å¯ç”¨åˆ—ï¼Œå·²è·³è¿‡")
            return False
        self.logger.info(f"ğŸ¯ é€‰æ‹©æ€§åˆ—è¦†ç›–åŒæ­¥: {columns}")

        # æ„å»ºç´¢å¼•
        current_index = self.converter.build_data_index(
            current_df, self.config.index_column
        )

        # å‡†å¤‡æ•°æ®æ˜ å°„ {row_idx: {col: value}}
        update_data_map: Dict[int, Dict[str, Any]] = {}  # æ›´æ–°ç°æœ‰è¡Œçš„æŒ‡å®šåˆ—
        new_rows: List[pd.Series] = []  # å…¨æ–°çš„è¡Œ

        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(
                row, self.config.index_column
            )
            if index_hash and index_hash in current_index:
                # è¦†ç›–ç°æœ‰è¡Œçš„æŒ‡å®šåˆ—
                current_row_idx = current_index[index_hash]
                if current_row_idx not in update_data_map:
                    update_data_map[current_row_idx] = {}

                # åªè¦†ç›–æŒ‡å®šåˆ—
                for col in columns:
                    if col in df.columns:
                        update_data_map[current_row_idx][col] = row[col]
            else:
                # å…¨æ–°è¡Œ
                new_rows.append(row)

        self.logger.info(
            f"é€‰æ‹©æ€§åˆ—è¦†ç›–è®¡åˆ’: è¦†ç›– {len(update_data_map)} è¡Œçš„æŒ‡å®šåˆ—ï¼Œæ–°å¢ {len(new_rows)} è¡Œ"
        )

        success = True

        # æ‰§è¡Œé€‰æ‹©æ€§åˆ—è¦†ç›–æ›´æ–°
        if update_data_map:
            success = self._update_selective_columns(current_df, update_data_map)

        # è¿½åŠ æ–°è¡Œï¼ˆå¦‚æœæœ‰ï¼‰
        if new_rows and success:
            # å¯¹äºæ–°è¡Œï¼Œä¹Ÿåº”è¯¥åªåŒ…å«é€‰æ‹©æ€§åˆ—
            if (
                self.config.selective_sync.enabled
                and self.config.selective_sync.columns
            ):
                success = self._append_selective_columns(pd.DataFrame(new_rows))
            else:
                # å¸¸è§„è¿½åŠ 
                new_df = pd.DataFrame(new_rows)
                new_values = self.converter.df_to_values(new_df, include_headers=False)

                if (
                    isinstance(self.api, SheetAPI)
                    and self.config.spreadsheet_token
                    and self.config.sheet_id
                ):
                    success = self.api.append_sheet_data(
                        self.config.spreadsheet_token,
                        self.config.sheet_id,
                        new_values,
                        self.config.batch_size,
                        self.config.rate_limit_delay,
                    )

        return success

    def sync_clone(self, df: pd.DataFrame) -> bool:
        """å…‹éš†åŒæ­¥ï¼šæ¸…ç©ºå…¨éƒ¨ï¼Œç„¶åæ–°å¢å…¨éƒ¨"""
        self.logger.info("å¼€å§‹å…‹éš†åŒæ­¥...")

        if self.config.target_type == TargetType.BITABLE:
            return self._sync_clone_bitable(df)
        else:  # SHEET
            return self._sync_clone_sheet(df)

    def _sync_clone_bitable(self, df: pd.DataFrame) -> bool:
        """å¤šç»´è¡¨æ ¼å…‹éš†åŒæ­¥"""
        # è·å–æ‰€æœ‰ç°æœ‰è®°å½•ï¼ˆä»…è·å–æœ€å°å­—æ®µé›†ï¼Œcloneæ¨¡å¼åªéœ€record_idï¼‰
        fetch_fields = self._get_bitable_fetch_field_names(df, "clone")
        existing_records = self.get_all_bitable_records(field_names=fetch_fields)
        existing_record_ids = [record["record_id"] for record in existing_records]

        self.logger.info(
            f"å…‹éš†åŒæ­¥è®¡åˆ’: åˆ é™¤ {len(existing_record_ids)} æ¡å·²æœ‰è®°å½•ï¼Œç„¶åæ–°å¢ {len(df)} æ¡è®°å½•"
        )

        # åˆ é™¤æ‰€æœ‰è®°å½•
        delete_success = True
        if (
            existing_record_ids
            and isinstance(self.api, BitableAPI)
            and self.config.app_token
            and self.config.table_id
        ):
            delete_success = self.process_in_batches(
                existing_record_ids,
                self.config.batch_size,
                self.api.batch_delete_records,
                self.config.app_token,
                self.config.table_id,
            )

        # æ–°å¢å…¨éƒ¨è®°å½•
        field_types = self.get_field_types()
        new_records = self.converter.df_to_records(df, field_types)
        create_success = False
        if (
            isinstance(self.api, BitableAPI)
            and self.config.app_token
            and self.config.table_id
        ):
            create_success = self.process_in_batches(
                new_records,
                self.config.batch_size,
                self.api.batch_create_records,
                self.config.app_token,
                self.config.table_id,
            )

        return delete_success and create_success

    def _sync_clone_sheet(self, df: pd.DataFrame) -> bool:
        """ç”µå­è¡¨æ ¼å…‹éš†åŒæ­¥ - ä½¿ç”¨ä¼˜åŒ–APIç­–ç•¥"""
        # è½¬æ¢æ•°æ®æ ¼å¼
        values = self.converter.df_to_values(df)

        self.logger.info(f"å…‹éš†åŒæ­¥è®¡åˆ’: æ¸…ç©ºç°æœ‰æ•°æ®ï¼Œæ–°å¢ {len(df)} è¡Œ")
        self.logger.info("ä½¿ç”¨write_sheet_dataè¿›è¡Œå…‹éš†å†™å…¥")

        # é¦–å…ˆæ¸…ç©ºè¡¨æ ¼çš„ä¸€ä¸ªå¤§èŒƒå›´
        if (
            isinstance(self.api, SheetAPI)
            and self.config.spreadsheet_token
            and self.config.sheet_id
        ):
            self.logger.info("æ¸…ç©ºç°æœ‰æ•°æ®...")
            clear_range = self._build_sheet_full_range()
            if not clear_range:
                self.logger.error("æ— æ³•è·å–å·¥ä½œè¡¨ç½‘æ ¼èŒƒå›´ï¼Œæ¸…ç©ºå¤±è´¥")
                return False
            clear_success = self.api.clear_sheet_data(
                self.config.spreadsheet_token, self.config.sheet_id, clear_range
            )
            if not clear_success:
                self.logger.error("æ¸…ç©ºç”µå­è¡¨æ ¼å¤±è´¥ï¼Œç»ˆæ­¢å…‹éš†åŒæ­¥")
                return False

            # ä½¿ç”¨å¢å¼ºçš„å†™å…¥æ–¹æ³•
            write_success = self.api.write_sheet_data(
                self.config.spreadsheet_token,
                self.config.sheet_id,
                values,
                self.config.batch_size,
                80,  # col_batch_size
                self.config.rate_limit_delay,
            )
        else:
            write_success = False

        # æ•°æ®å†™å…¥æˆåŠŸåï¼Œå†åº”ç”¨æ™ºèƒ½å­—æ®µé…ç½®
        if write_success:
            if not self._setup_sheet_intelligence(df):
                self.logger.warning("æ™ºèƒ½å­—æ®µé…ç½®å¤±è´¥ï¼Œä½†æ•°æ®åŒæ­¥å·²å®Œæˆ")

        return write_success

    def _setup_sheet_intelligence(self, df: pd.DataFrame) -> bool:
        """
        ä¸ºç”µå­è¡¨æ ¼è®¾ç½®æ™ºèƒ½å­—æ®µé…ç½®

        Args:
            df: æ•°æ®DataFrame

        Returns:
            æ˜¯å¦è®¾ç½®æˆåŠŸ
        """
        if self.config.target_type != TargetType.SHEET:
            return True

        if not isinstance(self.api, SheetAPI):
            self.logger.error(
                "å†…éƒ¨é€»è¾‘é”™è¯¯: _setup_sheet_intelligence åº”è¯¥åªè¢« SheetAPI è°ƒç”¨"
            )
            return False

        # ä¸åŒç­–ç•¥çš„é…ç½®èŒƒå›´ä¸åŒ
        strategy_name = self.config.field_type_strategy.value
        self.logger.info(f"å¼€å§‹ç”µå­è¡¨æ ¼æ™ºèƒ½å­—æ®µé…ç½® ({strategy_name}ç­–ç•¥)...")

        # rawç­–ç•¥ï¼šä¸åº”ç”¨ä»»ä½•æ ¼å¼åŒ–ï¼Œç›´æ¥è¿”å›æˆåŠŸ
        if strategy_name == "raw":
            self.logger.info("rawç­–ç•¥ï¼šè·³è¿‡æ‰€æœ‰æ ¼å¼åŒ–ï¼Œä¿æŒåŸå§‹æ•°æ®")
            return True

        # ç”Ÿæˆå­—æ®µé…ç½®
        field_config = self.converter.generate_sheet_field_config(
            df, self.config.field_type_strategy.value, self.config
        )

        success = True

        # 1. é…ç½®ä¸‹æ‹‰åˆ—è¡¨ (baseç­–ç•¥è·³è¿‡)
        if strategy_name != "base":
            for dropdown_config in field_config["dropdown_configs"]:
                column_name = dropdown_config["column"]

                # è®¡ç®—åˆ—çš„ç»å¯¹ä½ç½®
                start_col_num = self.api.column_letter_to_number(
                    self.config.start_column
                )
                col_index_in_df = list(df.columns).index(column_name)
                actual_col_num = start_col_num + col_index_in_df
                col_letter = self.api.column_number_to_letter(actual_col_num)

                # è®¡ç®—è¡Œçš„ç»å¯¹èŒƒå›´ (æ•°æ®è¡Œï¼Œä¸å«è¡¨å¤´)
                start_data_row = self.config.start_row + 1
                end_data_row = self.config.start_row + len(df)

                # ä»…åœ¨æœ‰æ•°æ®è¡Œæ—¶æ‰è®¾ç½®èŒƒå›´
                if end_data_row >= start_data_row:
                    range_str = f"{self.config.sheet_id}!{col_letter}{start_data_row}:{col_letter}{end_data_row}"
                else:
                    self.logger.warning(
                        f"åˆ— '{column_name}' æ²¡æœ‰æ•°æ®è¡Œï¼Œè·³è¿‡ä¸‹æ‹‰åˆ—è¡¨è®¾ç½®"
                    )
                    continue

                # ç¡®ä¿ä½¿ç”¨SheetAPIå¹¶æ£€æŸ¥token
                if not isinstance(self.api, SheetAPI):
                    self.logger.error("APIç±»å‹ä¸åŒ¹é…ï¼Œéœ€è¦SheetAPI")
                    continue

                if not self.config.spreadsheet_token:
                    self.logger.error("ç”µå­è¡¨æ ¼Tokenä¸ºç©º")
                    continue

                # è®¾ç½®ä¸‹æ‹‰åˆ—è¡¨
                dropdown_success = self.api.set_dropdown_validation(
                    self.config.spreadsheet_token,
                    range_str,
                    dropdown_config["options"],
                    dropdown_config["multiple"],
                    dropdown_config["colors"],
                )

                if dropdown_success:
                    self.logger.info(f"æˆåŠŸä¸ºåˆ— '{column_name}' è®¾ç½®ä¸‹æ‹‰åˆ—è¡¨")
                else:
                    self.logger.error(f"ä¸ºåˆ— '{column_name}' è®¾ç½®ä¸‹æ‹‰åˆ—è¡¨å¤±è´¥")
                    # ä¸è®¾ç½®success = Falseï¼Œå…è®¸ç»§ç»­å…¶ä»–åˆ—çš„æ“ä½œ
        else:
            self.logger.info("baseç­–ç•¥è·³è¿‡ä¸‹æ‹‰åˆ—è¡¨é…ç½®")

        # 2. é…ç½®æ—¥æœŸæ ¼å¼
        if (
            field_config["date_columns"]
            and isinstance(self.api, SheetAPI)
            and self.config.spreadsheet_token
        ):
            date_ranges = []
            for column_name in field_config["date_columns"]:
                start_col_num = self.api.column_letter_to_number(
                    self.config.start_column
                )
                col_index_in_df = list(df.columns).index(column_name)
                actual_col_num = start_col_num + col_index_in_df
                col_letter = self.api.column_number_to_letter(actual_col_num)

                start_data_row = self.config.start_row + 1
                end_data_row = self.config.start_row + len(df)

                if end_data_row >= start_data_row:
                    range_str = f"{self.config.sheet_id}!{col_letter}{start_data_row}:{col_letter}{end_data_row}"
                    date_ranges.append(range_str)

            # è®¾ç½®æ—¥æœŸæ ¼å¼
            date_success = self.api.set_date_format(
                self.config.spreadsheet_token, date_ranges, "yyyy/MM/dd"
            )

            if date_success:
                self.logger.info(f"æˆåŠŸä¸º {len(date_ranges)} ä¸ªæ—¥æœŸåˆ—è®¾ç½®æ ¼å¼")
            else:
                self.logger.error("è®¾ç½®æ—¥æœŸæ ¼å¼å¤±è´¥")
                # ä¸è®¾ç½®success = Falseï¼Œå…è®¸ç»§ç»­å…¶ä»–æ“ä½œ

        # 3. é…ç½®æ•°å­—æ ¼å¼
        if (
            field_config["number_columns"]
            and isinstance(self.api, SheetAPI)
            and self.config.spreadsheet_token
        ):
            number_ranges = []
            for column_name in field_config["number_columns"]:
                start_col_num = self.api.column_letter_to_number(
                    self.config.start_column
                )
                col_index_in_df = list(df.columns).index(column_name)
                actual_col_num = start_col_num + col_index_in_df
                col_letter = self.api.column_number_to_letter(actual_col_num)

                start_data_row = self.config.start_row + 1
                end_data_row = self.config.start_row + len(df)

                if end_data_row >= start_data_row:
                    range_str = f"{self.config.sheet_id}!{col_letter}{start_data_row}:{col_letter}{end_data_row}"
                    number_ranges.append(range_str)

            # è®¾ç½®æ•°å­—æ ¼å¼
            number_success = self.api.set_number_format(
                self.config.spreadsheet_token, number_ranges, "#,##0.00"
            )

            if number_success:
                self.logger.info(f"æˆåŠŸä¸º {len(number_ranges)} ä¸ªæ•°å­—åˆ—è®¾ç½®æ ¼å¼")
            else:
                self.logger.error("è®¾ç½®æ•°å­—æ ¼å¼å¤±è´¥")
                # ä¸è®¾ç½®success = Falseï¼Œå…è®¸ç»§ç»­å…¶ä»–æ“ä½œ

        # è¾“å‡ºé…ç½®æ‘˜è¦
        dropdown_count = (
            len(field_config["dropdown_configs"]) if strategy_name != "base" else 0
        )
        date_count = len(field_config["date_columns"])
        number_count = len(field_config["number_columns"])
        total_configs = dropdown_count + date_count + number_count

        if total_configs > 0:
            config_summary = []
            if dropdown_count > 0:
                config_summary.append(f"{dropdown_count}ä¸ªä¸‹æ‹‰åˆ—è¡¨")
            if date_count > 0:
                config_summary.append(f"{date_count}ä¸ªæ—¥æœŸæ ¼å¼")
            if number_count > 0:
                config_summary.append(f"{number_count}ä¸ªæ•°å­—æ ¼å¼")

            self.logger.info(f"æ™ºèƒ½å­—æ®µé…ç½®å®Œæˆ: {', '.join(config_summary)}")
        else:
            self.logger.info("æœªæ£€æµ‹åˆ°éœ€è¦æ™ºèƒ½é…ç½®çš„å­—æ®µ")

        return success

    def sync(self, df: pd.DataFrame) -> bool:
        """æ‰§è¡ŒåŒæ­¥"""
        target_name = (
            "å¤šç»´è¡¨æ ¼" if self.config.target_type == TargetType.BITABLE else "ç”µå­è¡¨æ ¼"
        )
        self.logger.info(
            f"å¼€å§‹æ‰§è¡Œ {target_name} {self.config.sync_mode.value} åŒæ­¥æ¨¡å¼"
        )
        self.logger.info(f"æ•°æ®æº: {len(df)} è¡Œ x {len(df.columns)} åˆ—")

        # é‡ç½®è½¬æ¢ç»Ÿè®¡
        self.converter.reset_stats()

        # é€‰æ‹©æ€§åŒæ­¥å‰ç½®è¿‡æ»¤ï¼ˆå½±å“å­—æ®µåˆ›å»º/ç½®ä¿¡åº¦åˆ†æèŒƒå›´ï¼‰
        if self.config.selective_sync.enabled:
            df = self._apply_selective_filter(df)

        # å¤šç»´è¡¨æ ¼æ¨¡å¼éœ€è¦ç¡®ä¿å­—æ®µå­˜åœ¨
        if self.config.target_type == TargetType.BITABLE:
            success, field_types = self.ensure_fields_exist(df)
            if not success:
                self.logger.error("å­—æ®µåˆ›å»ºå¤±è´¥ï¼ŒåŒæ­¥ç»ˆæ­¢")
                return False

            self.logger.info(f"è·å–åˆ° {len(field_types)} ä¸ªå­—æ®µçš„ç±»å‹ä¿¡æ¯")

            # æ˜¾ç¤ºå­—æ®µç±»å‹æ˜ å°„æ‘˜è¦
            self._show_field_analysis_summary(df, field_types)

            # é¢„æ£€æŸ¥ï¼šåˆ†ææ•°æ®ä¸å­—æ®µç±»å‹çš„åŒ¹é…æƒ…å†µ
            self.logger.info("\nğŸ” æ­£åœ¨åˆ†ææ•°æ®ä¸å­—æ®µç±»å‹åŒ¹é…æƒ…å†µ...")
            mismatch_warnings = []
            sample_size = min(50, len(df))  # æ£€æŸ¥å‰50è¡Œä½œä¸ºæ ·æœ¬

            for _, row in df.head(sample_size).iterrows():
                for col_name, value in row.to_dict().items():
                    if pd.notnull(value) and col_name in field_types:
                        field_type = field_types[col_name]
                        # ç®€å•çš„ç±»å‹ä¸åŒ¹é…æ£€æµ‹
                        if field_type == 2 and isinstance(
                            value, str
                        ):  # æ•°å­—å­—æ®µä½†æ˜¯å­—ç¬¦ä¸²å€¼
                            if not self.converter._is_number_string(str(value).strip()):
                                mismatch_warnings.append(
                                    f"å­—æ®µ '{col_name}' æ˜¯æ•°å­—ç±»å‹ï¼Œä½†åŒ…å«éæ•°å­—å€¼: '{value}'"
                                )
                        elif field_type == 5 and isinstance(
                            value, str
                        ):  # æ—¥æœŸå­—æ®µä½†æ˜¯å­—ç¬¦ä¸²å€¼
                            if not (
                                self.converter._is_timestamp_string(str(value))
                                or self.converter._is_date_string(str(value))
                            ):
                                mismatch_warnings.append(
                                    f"å­—æ®µ '{col_name}' æ˜¯æ—¥æœŸç±»å‹ï¼Œä½†åŒ…å«éæ—¥æœŸå€¼: '{value}'"
                                )

            if mismatch_warnings:
                unique_warnings = list(
                    set(mismatch_warnings[:10])
                )  # æ˜¾ç¤ºå‰10ä¸ªå”¯ä¸€è­¦å‘Š
                self.logger.warning(
                    f"å‘ç° {len(set(mismatch_warnings))} ç§æ•°æ®ç±»å‹ä¸åŒ¹é…æƒ…å†µï¼ˆæ ·æœ¬æ£€æŸ¥ï¼‰:"
                )
                for warning in unique_warnings:
                    self.logger.warning(f"  â€¢ {warning}")
                self.logger.info("ç¨‹åºå°†è‡ªåŠ¨è¿›è¡Œå¼ºåˆ¶ç±»å‹è½¬æ¢...")
            else:
                self.logger.info("âœ… æ•°æ®ç±»å‹åŒ¹é…è‰¯å¥½")

        # æ ¹æ®åŒæ­¥æ¨¡å¼æ‰§è¡Œå¯¹åº”æ“ä½œ
        sync_result = False
        if self.config.sync_mode == SyncMode.FULL:
            sync_result = self.sync_full(df)
        elif self.config.sync_mode == SyncMode.INCREMENTAL:
            sync_result = self.sync_incremental(df)
        elif self.config.sync_mode == SyncMode.OVERWRITE:
            sync_result = self.sync_overwrite(df)
        elif self.config.sync_mode == SyncMode.CLONE:
            sync_result = self.sync_clone(df)
        else:
            self.logger.error(f"ä¸æ”¯æŒçš„åŒæ­¥æ¨¡å¼: {self.config.sync_mode}")
            return False

        # è¾“å‡ºè½¬æ¢ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…å¤šç»´è¡¨æ ¼æ¨¡å¼ï¼‰
        if self.config.target_type == TargetType.BITABLE:
            self.converter.report_conversion_stats()

        return sync_result

    def _show_field_analysis_summary(
        self, df: pd.DataFrame, field_types: Dict[str, int]
    ):
        """æ˜¾ç¤ºå­—æ®µåˆ†ææ‘˜è¦"""
        self.logger.info("\nğŸ“‹ å­—æ®µç±»å‹æ˜ å°„æ‘˜è¦:")
        self.logger.info("-" * 50)

        for col_name in df.columns:
            if col_name in field_types:
                field_type = field_types[col_name]
                type_name = self.converter.get_field_type_name(field_type)
                self.logger.info(f"  {col_name} â†’ {type_name} (ç±»å‹ç : {field_type})")
            else:
                self.logger.warning(f"  {col_name} â†’ æœªçŸ¥å­—æ®µç±»å‹")

        self.logger.info("-" * 50)

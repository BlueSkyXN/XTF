#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒæ­¥å¼•æ“æ¨¡å—
æä¾›XTFåŒæ­¥å¼•æ“ï¼Œæ”¯æŒå››ç§åŒæ­¥æ¨¡å¼çš„æ™ºèƒ½åŒæ­¥
"""

import pandas as pd
import time
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple

from .config import SyncConfig, SyncMode
from .converter import DataConverter
from api import FeishuAuth, RetryableAPIClient, BitableAPI, RateLimiter


class XTFSyncEngine:
    """XTFåŒæ­¥å¼•æ“ - æ”¯æŒå››ç§åŒæ­¥æ¨¡å¼çš„æ™ºèƒ½åŒæ­¥"""
    
    def __init__(self, config: SyncConfig):
        """
        åˆå§‹åŒ–åŒæ­¥å¼•æ“
        
        Args:
            config: åŒæ­¥é…ç½®å¯¹è±¡
        """
        self.config = config
        
        # åˆå§‹åŒ–APIç»„ä»¶
        self.auth = FeishuAuth(config.app_id, config.app_secret)
        self.api_client = RetryableAPIClient(
            max_retries=config.max_retries,
            rate_limiter=RateLimiter(config.rate_limit_delay)
        )
        self.bitable_api = BitableAPI(self.auth, self.api_client)
        
        # åˆå§‹åŒ–æ•°æ®è½¬æ¢å™¨
        self.converter = DataConverter()
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        self.logger = logging.getLogger(__name__)
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = Path('logs')
        log_dir.mkdir(exist_ok=True)
        
        log_file = log_dir / f"xtf_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        # æ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨
        logging.getLogger().handlers.clear()
        
        level = getattr(logging, self.config.log_level.upper(), logging.INFO)
        logging.basicConfig(
            level=level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
    
    def get_field_types(self) -> Dict[str, int]:
        """è·å–å­—æ®µç±»å‹æ˜ å°„"""
        try:
            existing_fields = self.bitable_api.list_fields(self.config.app_token, self.config.table_id)
            field_types = {}
            for field in existing_fields:
                field_name = field.get('field_name', '')
                field_type = field.get('type', 1)  # é»˜è®¤ä¸ºæ–‡æœ¬ç±»å‹
                field_types[field_name] = field_type
            
            self.logger.debug(f"è·å–åˆ° {len(field_types)} ä¸ªå­—æ®µç±»å‹ä¿¡æ¯")
            return field_types
            
        except Exception as e:
            self.logger.warning(f"è·å–å­—æ®µç±»å‹å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æ™ºèƒ½ç±»å‹æ£€æµ‹")
            return {}

    def ensure_fields_exist(self, df: pd.DataFrame) -> Tuple[bool, Dict[str, int]]:
        """ç¡®ä¿æ‰€éœ€å­—æ®µå­˜åœ¨äºç›®æ ‡è¡¨ä¸­ï¼Œè¿”å›æˆåŠŸçŠ¶æ€å’Œå­—æ®µç±»å‹æ˜ å°„"""
        try:
            # è·å–ç°æœ‰å­—æ®µ
            existing_fields = self.bitable_api.list_fields(self.config.app_token, self.config.table_id)
            existing_field_names = {field['field_name'] for field in existing_fields}
            
            # æ„å»ºå­—æ®µç±»å‹æ˜ å°„
            field_types = {}
            for field in existing_fields:
                field_name = field.get('field_name', '')
                field_type = field.get('type', 1)
                field_types[field_name] = field_type
            
            if self.config.create_missing_fields:
                # æ‰¾å‡ºç¼ºå¤±çš„å­—æ®µ
                required_fields = set(df.columns)
                missing_fields = required_fields - existing_field_names
                
                if missing_fields:
                    self.logger.info(f"éœ€è¦åˆ›å»º {len(missing_fields)} ä¸ªç¼ºå¤±å­—æ®µ: {', '.join(missing_fields)}")
                    
                    # åˆ†ææ¯ä¸ªç¼ºå¤±å­—æ®µçš„æ•°æ®ç‰¹å¾å¹¶åˆ›å»ºåˆé€‚ç±»å‹çš„å­—æ®µ
                    for field_name in missing_fields:
                        analysis = self.converter.analyze_excel_column_data(df, field_name)
                        suggested_type = analysis['suggested_feishu_type']
                        confidence = analysis['confidence']
                        
                        self.logger.info(f"å­—æ®µ '{field_name}': {analysis['analysis']}, "
                                       f"å»ºè®®ç±»å‹: {self.converter.get_field_type_name(suggested_type)} "
                                       f"(ç½®ä¿¡åº¦: {confidence:.1%})")
                        
                        success = self.bitable_api.create_field(
                            self.config.app_token, 
                            self.config.table_id, 
                            field_name,
                            suggested_type
                        )
                        if not success:
                            return False, field_types
                        
                        # è®°å½•æ–°åˆ›å»ºå­—æ®µçš„ç±»å‹
                        field_types[field_name] = suggested_type
                    
                    # ç­‰å¾…å­—æ®µåˆ›å»ºå®Œæˆ
                    time.sleep(2)
                else:
                    self.logger.info("æ‰€æœ‰å¿…éœ€å­—æ®µå·²å­˜åœ¨")
            
            return True, field_types
            
        except Exception as e:
            self.logger.error(f"å­—æ®µæ£€æŸ¥å¤±è´¥: {e}")
            return False, {}
    
    def process_in_batches(self, items: List[Any], batch_size: int, 
                          processor_func, *args, **kwargs) -> bool:
        """åˆ†æ‰¹å¤„ç†æ•°æ®"""
        total_batches = (len(items) + batch_size - 1) // batch_size
        success_count = 0
        
        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            batch_num = i // batch_size + 1
            
            try:
                # ä¿®å¤å‚æ•°ä¼ é€’é¡ºåºï¼šå…ˆä¼ é€’å›ºå®šå‚æ•°ï¼Œå†ä¼ é€’æ‰¹æ¬¡æ•°æ®
                if processor_func(*args, batch, **kwargs):
                    success_count += 1
                    self.logger.info(f"æ‰¹æ¬¡ {batch_num}/{total_batches} å¤„ç†æˆåŠŸ ({len(batch)} æ¡è®°å½•)")
                else:
                    self.logger.error(f"æ‰¹æ¬¡ {batch_num}/{total_batches} å¤„ç†å¤±è´¥")
            except Exception as e:
                self.logger.error(f"æ‰¹æ¬¡ {batch_num}/{total_batches} å¤„ç†å¼‚å¸¸: {e}")
        
        self.logger.info(f"æ‰¹å¤„ç†å®Œæˆ: {success_count}/{total_batches} ä¸ªæ‰¹æ¬¡æˆåŠŸ")
        return success_count == total_batches
        
    def sync_full(self, df: pd.DataFrame, field_types: Optional[Dict[str, int]] = None) -> bool:
        """å…¨é‡åŒæ­¥ï¼šå·²å­˜åœ¨ç´¢å¼•å€¼çš„æ›´æ–°ï¼Œä¸å­˜åœ¨çš„æ–°å¢"""
        self.logger.info("å¼€å§‹å…¨é‡åŒæ­¥...")
        
        if not self.config.index_column:
            self.logger.warning("æœªæŒ‡å®šç´¢å¼•åˆ—ï¼Œå°†æ‰§è¡Œçº¯æ–°å¢æ“ä½œ")
            new_records = self.converter.df_to_records(df, field_types)
            return self.process_in_batches(
                new_records, self.config.batch_size,
                self.bitable_api.batch_create_records,
                self.config.app_token, self.config.table_id
            )
        
        # è·å–ç°æœ‰è®°å½•å¹¶å»ºç«‹ç´¢å¼•
        existing_records = self.bitable_api.get_all_records(self.config.app_token, self.config.table_id)
        existing_index = self.converter.build_record_index(existing_records, self.config.index_column)
        
        # åˆ†ç±»æœ¬åœ°æ•°æ®
        records_to_update = []
        records_to_create = []
        
        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(row, self.config.index_column)
            
            # ä½¿ç”¨å­—æ®µç±»å‹è½¬æ¢æ„å»ºè®°å½•
            fields = {}
            for k, v in row.to_dict().items():
                if pd.notnull(v):
                    converted_value = self.converter.convert_field_value_safe(str(k), v, field_types)
                    if converted_value is not None:
                        fields[str(k)] = converted_value
            
            record = {"fields": fields}
            
            if index_hash and index_hash in existing_index:
                # éœ€è¦æ›´æ–°çš„è®°å½•
                existing_record = existing_index[index_hash]
                record["record_id"] = existing_record["record_id"]
                records_to_update.append(record)
            else:
                # éœ€è¦æ–°å¢çš„è®°å½•
                records_to_create.append(record)
        
        self.logger.info(f"å…¨é‡åŒæ­¥è®¡åˆ’: æ›´æ–° {len(records_to_update)} æ¡ï¼Œæ–°å¢ {len(records_to_create)} æ¡")
        
        # æ‰§è¡Œæ›´æ–°
        update_success = True
        if records_to_update:
            update_success = self.process_in_batches(
                records_to_update, self.config.batch_size,
                self.bitable_api.batch_update_records,
                self.config.app_token, self.config.table_id
            )
        
        # æ‰§è¡Œæ–°å¢
        create_success = True
        if records_to_create:
            create_success = self.process_in_batches(
                records_to_create, self.config.batch_size,
                self.bitable_api.batch_create_records,
                self.config.app_token, self.config.table_id
            )
        
        return update_success and create_success
    
    def sync_incremental(self, df: pd.DataFrame, field_types: Optional[Dict[str, int]] = None) -> bool:
        """å¢é‡åŒæ­¥ï¼šåªæ–°å¢ä¸å­˜åœ¨ç´¢å¼•å€¼çš„è®°å½•"""
        self.logger.info("å¼€å§‹å¢é‡åŒæ­¥...")
        
        if not self.config.index_column:
            self.logger.warning("æœªæŒ‡å®šç´¢å¼•åˆ—ï¼Œå°†æ‰§è¡Œçº¯æ–°å¢æ“ä½œ")
            new_records = self.converter.df_to_records(df, field_types)
            return self.process_in_batches(
                new_records, self.config.batch_size,
                self.bitable_api.batch_create_records,
                self.config.app_token, self.config.table_id
            )
        
        # è·å–ç°æœ‰è®°å½•å¹¶å»ºç«‹ç´¢å¼•
        existing_records = self.bitable_api.get_all_records(self.config.app_token, self.config.table_id)
        existing_index = self.converter.build_record_index(existing_records, self.config.index_column)
        
        # ç­›é€‰å‡ºéœ€è¦æ–°å¢çš„è®°å½•
        records_to_create = []
        
        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(row, self.config.index_column)
            
            if not index_hash or index_hash not in existing_index:
                # ä½¿ç”¨å­—æ®µç±»å‹è½¬æ¢æ„å»ºè®°å½•
                fields = {}
                for k, v in row.to_dict().items():
                    if pd.notnull(v):
                        converted_value = self.converter.convert_field_value_safe(str(k), v, field_types)
                        if converted_value is not None:
                            fields[str(k)] = converted_value
                
                record = {"fields": fields}
                records_to_create.append(record)
        
        self.logger.info(f"å¢é‡åŒæ­¥è®¡åˆ’: æ–°å¢ {len(records_to_create)} æ¡è®°å½•")
        
        if records_to_create:
            return self.process_in_batches(
                records_to_create, self.config.batch_size,
                self.bitable_api.batch_create_records,
                self.config.app_token, self.config.table_id
            )
        else:
            self.logger.info("æ²¡æœ‰æ–°è®°å½•éœ€è¦åŒæ­¥")
            return True
    
    def sync_overwrite(self, df: pd.DataFrame, field_types: Optional[Dict[str, int]] = None) -> bool:
        """è¦†ç›–åŒæ­¥ï¼šåˆ é™¤å·²å­˜åœ¨ç´¢å¼•å€¼çš„è®°å½•ï¼Œç„¶åæ–°å¢å…¨éƒ¨è®°å½•"""
        self.logger.info("å¼€å§‹è¦†ç›–åŒæ­¥...")
        
        if not self.config.index_column:
            self.logger.error("è¦†ç›–åŒæ­¥æ¨¡å¼éœ€è¦æŒ‡å®šç´¢å¼•åˆ—")
            return False
        
        # è·å–ç°æœ‰è®°å½•å¹¶å»ºç«‹ç´¢å¼•
        existing_records = self.bitable_api.get_all_records(self.config.app_token, self.config.table_id)
        existing_index = self.converter.build_record_index(existing_records, self.config.index_column)
        
        # æ‰¾å‡ºéœ€è¦åˆ é™¤çš„è®°å½•
        record_ids_to_delete = []
        
        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(row, self.config.index_column)
            if index_hash and index_hash in existing_index:
                existing_record = existing_index[index_hash]
                record_ids_to_delete.append(existing_record["record_id"])
        
        self.logger.info(f"è¦†ç›–åŒæ­¥è®¡åˆ’: åˆ é™¤ {len(record_ids_to_delete)} æ¡å·²å­˜åœ¨è®°å½•ï¼Œç„¶åæ–°å¢ {len(df)} æ¡è®°å½•")
        
        # åˆ é™¤å·²å­˜åœ¨çš„è®°å½•
        delete_success = True
        if record_ids_to_delete:
            delete_success = self.process_in_batches(
                record_ids_to_delete, self.config.batch_size,
                self.bitable_api.batch_delete_records,
                self.config.app_token, self.config.table_id
            )
        
        # æ–°å¢å…¨éƒ¨è®°å½•
        new_records = self.converter.df_to_records(df, field_types)
        create_success = self.process_in_batches(
            new_records, self.config.batch_size,
            self.bitable_api.batch_create_records,
            self.config.app_token, self.config.table_id
        )
        
        return delete_success and create_success
    
    def sync_clone(self, df: pd.DataFrame, field_types: Optional[Dict[str, int]] = None) -> bool:
        """å…‹éš†åŒæ­¥ï¼šæ¸…ç©ºå…¨éƒ¨å·²æœ‰è®°å½•ï¼Œç„¶åæ–°å¢å…¨éƒ¨è®°å½•"""
        self.logger.info("å¼€å§‹å…‹éš†åŒæ­¥...")
        
        # è·å–æ‰€æœ‰ç°æœ‰è®°å½•
        existing_records = self.bitable_api.get_all_records(self.config.app_token, self.config.table_id)
        existing_record_ids = [record["record_id"] for record in existing_records]
        
        self.logger.info(f"å…‹éš†åŒæ­¥è®¡åˆ’: åˆ é™¤ {len(existing_record_ids)} æ¡å·²æœ‰è®°å½•ï¼Œç„¶åæ–°å¢ {len(df)} æ¡è®°å½•")
        
        # åˆ é™¤æ‰€æœ‰è®°å½•
        delete_success = True
        if existing_record_ids:
            delete_success = self.process_in_batches(
                existing_record_ids, self.config.batch_size,
                self.bitable_api.batch_delete_records,
                self.config.app_token, self.config.table_id
            )
        
        # æ–°å¢å…¨éƒ¨è®°å½•
        new_records = self.converter.df_to_records(df, field_types)
        create_success = self.process_in_batches(
            new_records, self.config.batch_size,
            self.bitable_api.batch_create_records,
            self.config.app_token, self.config.table_id
        )
        
        return delete_success and create_success
    
    def sync(self, df: pd.DataFrame) -> bool:
        """æ‰§è¡ŒåŒæ­¥"""
        self.logger.info(f"å¼€å§‹æ‰§è¡Œ {self.config.sync_mode.value} åŒæ­¥æ¨¡å¼")
        self.logger.info(f"æ•°æ®æº: {len(df)} è¡Œ x {len(df.columns)} åˆ—")
        
        # é‡ç½®è½¬æ¢ç»Ÿè®¡
        self.converter.reset_stats()
        
        # ç¡®ä¿å­—æ®µå­˜åœ¨å¹¶è·å–å­—æ®µç±»å‹ä¿¡æ¯
        success, field_types = self.ensure_fields_exist(df)
        if not success:
            self.logger.error("å­—æ®µåˆ›å»ºå¤±è´¥ï¼ŒåŒæ­¥ç»ˆæ­¢")
            return False
        
        self.logger.info(f"è·å–åˆ° {len(field_types)} ä¸ªå­—æ®µçš„ç±»å‹ä¿¡æ¯")
        
        # æ˜¾ç¤ºå­—æ®µç±»å‹æ˜ å°„æ‘˜è¦
        self._show_field_analysis_summary(df, field_types)
        
        # é¢„æ£€æŸ¥ï¼šåˆ†ææ•°æ®ä¸å­—æ®µç±»å‹çš„åŒ¹é…æƒ…å†µ
        self.logger.info("\nğŸ” æ­£åœ¨åˆ†ææ•°æ®ä¸å­—æ®µç±»å‹åŒ¹é…æƒ…å†µ...")
        mismatch_warnings = []
        sample_size = min(50, len(df))  # æ£€æŸ¥å‰50è¡Œä½œä¸ºæ ·æœ¬
        
        for _, row in df.head(sample_size).iterrows():
            for col_name, value in row.to_dict().items():
                if pd.notnull(value) and col_name in field_types:
                    field_type = field_types[col_name]
                    # ç®€å•çš„ç±»å‹ä¸åŒ¹é…æ£€æµ‹
                    if field_type == 2 and isinstance(value, str):  # æ•°å­—å­—æ®µä½†æ˜¯å­—ç¬¦ä¸²å€¼
                        if not self.converter._is_number_string(str(value).strip()):
                            mismatch_warnings.append(f"å­—æ®µ '{col_name}' æ˜¯æ•°å­—ç±»å‹ï¼Œä½†åŒ…å«éæ•°å­—å€¼: '{value}'")
                    elif field_type == 5 and isinstance(value, str):  # æ—¥æœŸå­—æ®µä½†æ˜¯å­—ç¬¦ä¸²å€¼
                        if not (self.converter._is_timestamp_string(str(value)) or self.converter._is_date_string(str(value))):
                            mismatch_warnings.append(f"å­—æ®µ '{col_name}' æ˜¯æ—¥æœŸç±»å‹ï¼Œä½†åŒ…å«éæ—¥æœŸå€¼: '{value}'")
        
        if mismatch_warnings:
            unique_warnings = list(set(mismatch_warnings[:10]))  # æ˜¾ç¤ºå‰10ä¸ªå”¯ä¸€è­¦å‘Š
            self.logger.warning(f"å‘ç° {len(set(mismatch_warnings))} ç§æ•°æ®ç±»å‹ä¸åŒ¹é…æƒ…å†µï¼ˆæ ·æœ¬æ£€æŸ¥ï¼‰:")
            for warning in unique_warnings:
                self.logger.warning(f"  â€¢ {warning}")
            self.logger.info("ç¨‹åºå°†è‡ªåŠ¨è¿›è¡Œå¼ºåˆ¶ç±»å‹è½¬æ¢...")
        else:
            self.logger.info("âœ… æ•°æ®ç±»å‹åŒ¹é…è‰¯å¥½")
        
        # æ ¹æ®åŒæ­¥æ¨¡å¼æ‰§è¡Œå¯¹åº”æ“ä½œ
        sync_result = False
        if self.config.sync_mode == SyncMode.FULL:
            sync_result = self.sync_full(df, field_types)
        elif self.config.sync_mode == SyncMode.INCREMENTAL:
            sync_result = self.sync_incremental(df, field_types)
        elif self.config.sync_mode == SyncMode.OVERWRITE:
            sync_result = self.sync_overwrite(df, field_types)
        elif self.config.sync_mode == SyncMode.CLONE:
            sync_result = self.sync_clone(df, field_types)
        else:
            self.logger.error(f"ä¸æ”¯æŒçš„åŒæ­¥æ¨¡å¼: {self.config.sync_mode}")
            return False
        
        # è¾“å‡ºè½¬æ¢ç»Ÿè®¡ä¿¡æ¯
        self.converter.report_conversion_stats()
        
        return sync_result
    
    def _show_field_analysis_summary(self, df: pd.DataFrame, field_types: Dict[str, int]):
        """æ˜¾ç¤ºå­—æ®µåˆ†ææ‘˜è¦"""
        self.logger.info("\nğŸ“‹ å­—æ®µç±»å‹æ˜ å°„æ‘˜è¦:")
        self.logger.info("-" * 50)
        
        for col_name in df.columns:
            if col_name in field_types:
                field_type = field_types[col_name]
                type_name = self.converter.get_field_type_name(field_type)
                self.logger.info(f"  {col_name} â†’ {type_name} (ç±»å‹ç : {field_type})")
            else:
                self.logger.warning(f"  {col_name} â†’ æœªçŸ¥å­—æ®µç±»å‹")
                
        self.logger.info("-" * 50)
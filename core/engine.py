#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€åŒæ­¥å¼•æ“æ¨¡å—
æä¾›å¤šç»´è¡¨æ ¼å’Œç”µå­è¡¨æ ¼çš„ç»Ÿä¸€åŒæ­¥å¼•æ“
"""

import pandas as pd
import time
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List, Union, Tuple

from .config import SyncConfig, SyncMode, TargetType
from .converter import DataConverter
from api import FeishuAuth, RetryableAPIClient, BitableAPI, SheetAPI, RateLimiter


class XTFSyncEngine:
    """ç»Ÿä¸€åŒæ­¥å¼•æ“ - æ”¯æŒå¤šç»´è¡¨æ ¼å’Œç”µå­è¡¨æ ¼"""
    
    def __init__(self, config: SyncConfig):
        """
        åˆå§‹åŒ–åŒæ­¥å¼•æ“
        
        Args:
            config: ç»Ÿä¸€åŒæ­¥é…ç½®å¯¹è±¡
        """
        self.config = config
        
        # åˆå§‹åŒ–APIç»„ä»¶
        self.auth = FeishuAuth(config.app_id, config.app_secret)
        self.api_client = RetryableAPIClient(
            max_retries=config.max_retries,
            rate_limiter=RateLimiter(config.rate_limit_delay)
        )
        
        # æ ¹æ®ç›®æ ‡ç±»å‹é€‰æ‹©APIå®¢æˆ·ç«¯
        if config.target_type == TargetType.BITABLE:
            self.api: Union[BitableAPI, SheetAPI] = BitableAPI(self.auth, self.api_client)
        else:  # SHEET
            self.api: Union[BitableAPI, SheetAPI] = SheetAPI(self.auth, self.api_client)
        
        # åˆå§‹åŒ–æ•°æ®è½¬æ¢å™¨
        self.converter = DataConverter(config.target_type)
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        self.logger = logging.getLogger(__name__)
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = Path('logs')
        log_dir.mkdir(exist_ok=True)
        
        target_name = "bitable" if self.config.target_type == TargetType.BITABLE else "sheet"
        log_file = log_dir / f"xtf_{target_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        # æ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨
        logging.getLogger().handlers.clear()
        
        level = getattr(logging, self.config.log_level.upper(), logging.INFO)
        logging.basicConfig(
            level=level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
    
    # ========== å¤šç»´è¡¨æ ¼ä¸“ç”¨æ–¹æ³• ==========
    
    def get_field_types(self) -> Dict[str, int]:
        """è·å–å¤šç»´è¡¨æ ¼å­—æ®µç±»å‹æ˜ å°„"""
        if self.config.target_type != TargetType.BITABLE:
            return {}
            
        try:
            if not isinstance(self.api, BitableAPI):
                return {}
            existing_fields = self.api.list_fields(self.config.app_token, self.config.table_id)
            field_types = {}
            for field in existing_fields:
                field_name = field.get('field_name', '')
                field_type = field.get('type', 1)  # é»˜è®¤ä¸ºæ–‡æœ¬ç±»å‹
                field_types[field_name] = field_type
            
            self.logger.debug(f"è·å–åˆ° {len(field_types)} ä¸ªå­—æ®µç±»å‹ä¿¡æ¯")
            return field_types
            
        except Exception as e:
            self.logger.warning(f"è·å–å­—æ®µç±»å‹å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æ™ºèƒ½ç±»å‹æ£€æµ‹")
            return {}

    def ensure_fields_exist(self, df: pd.DataFrame) -> Tuple[bool, Dict[str, int]]:
        """ç¡®ä¿å¤šç»´è¡¨æ ¼æ‰€éœ€å­—æ®µå­˜åœ¨"""
        if self.config.target_type != TargetType.BITABLE:
            return True, {}
            
        try:
            # è·å–ç°æœ‰å­—æ®µ
            existing_fields = self.api.list_fields(self.config.app_token, self.config.table_id)
            existing_field_names = {field['field_name'] for field in existing_fields}
            
            # æ„å»ºå­—æ®µç±»å‹æ˜ å°„
            field_types = {}
            for field in existing_fields:
                field_name = field.get('field_name', '')
                field_type = field.get('type', 1)
                field_types[field_name] = field_type
            
            if self.config.create_missing_fields:
                # æ‰¾å‡ºç¼ºå¤±çš„å­—æ®µï¼Œä¿æŒåŸå§‹åˆ—é¡ºåº
                required_fields = set(df.columns)
                missing_fields_set = required_fields - existing_field_names
                
                # æŒ‰ç…§ DataFrame åˆ—çš„åŸå§‹é¡ºåºæ’åˆ—ç¼ºå¤±å­—æ®µ
                missing_fields = [col for col in df.columns if col in missing_fields_set]
                
                if missing_fields:
                    self.logger.info(f"æ£€æµ‹åˆ° {len(missing_fields)} ä¸ªç¼ºå¤±å­—æ®µ")
                    self.logger.info(f"ä½¿ç”¨å­—æ®µç±»å‹ç­–ç•¥: {self.config.field_type_strategy.value}")
                    
                    # åˆ†ææ¯ä¸ªç¼ºå¤±å­—æ®µ
                    creation_plan = []
                    for field_name in missing_fields:
                        # ä½¿ç”¨å¢å¼ºçš„åˆ†ææ–¹æ³•
                        analysis = self.converter.analyze_excel_column_data_enhanced(
                            df, field_name, self.config.field_type_strategy.value, self.config
                        )
                        
                        creation_plan.append({
                            'field_name': field_name,
                            'suggested_type': analysis['suggested_feishu_type'],
                            'confidence': analysis['confidence'],
                            'reason': analysis['recommendation_reason'],
                            'has_validation': analysis['has_excel_validation']
                        })
                    
                    # æ˜¾ç¤ºåˆ›å»ºè®¡åˆ’
                    self.logger.info("=" * 60)
                    self.logger.info("ğŸ“‹ å­—æ®µåˆ›å»ºè®¡åˆ’:")
                    for plan in creation_plan:
                        validation_mark = "ğŸ“‹" if plan['has_validation'] else "ğŸ“"
                        self.logger.info(
                            f"{validation_mark} {plan['field_name']}: "
                            f"{self.converter.get_field_type_name(plan['suggested_type'])} "
                            f"(ç½®ä¿¡åº¦: {plan['confidence']:.1%}) - {plan['reason']}"
                        )
                    self.logger.info("=" * 60)
                    
                    # æ‰§è¡Œå­—æ®µåˆ›å»º
                    for plan in creation_plan:
                        success = self.api.create_field(
                            self.config.app_token,
                            self.config.table_id,
                            plan['field_name'],
                            plan['suggested_type']
                        )
                        
                        if not success:
                            self.logger.error(f"å­—æ®µ '{plan['field_name']}' åˆ›å»ºå¤±è´¥")
                            return False, field_types
                        
                        # è®°å½•æ–°å­—æ®µç±»å‹
                        field_types[plan['field_name']] = plan['suggested_type']
                    
                    # ç­‰å¾…å­—æ®µåˆ›å»ºå®Œæˆ
                    import time
                    time.sleep(2)
                    
                else:
                    self.logger.info("âœ… æ‰€æœ‰å¿…éœ€å­—æ®µå·²å­˜åœ¨ï¼Œæ— éœ€åˆ›å»º")
            
            return True, field_types
            
        except Exception as e:
            self.logger.error(f"å­—æ®µæ£€æŸ¥å¤±è´¥: {e}")
            return False, {}
    
    def get_all_bitable_records(self) -> List[Dict]:
        """è·å–æ‰€æœ‰å¤šç»´è¡¨æ ¼è®°å½•"""
        if self.config.target_type != TargetType.BITABLE:
            return []
        return self.api.get_all_records(self.config.app_token, self.config.table_id)
    
    def process_in_batches(self, items: List[Any], batch_size: int, 
                          processor_func, *args, **kwargs) -> bool:
        """åˆ†æ‰¹å¤„ç†æ•°æ®ï¼ˆå¤šç»´è¡¨æ ¼æ¨¡å¼ï¼‰"""
        if self.config.target_type != TargetType.BITABLE:
            return False
            
        total_batches = (len(items) + batch_size - 1) // batch_size
        success_count = 0
        
        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            batch_num = i // batch_size + 1
            
            try:
                # ä¿®å¤å‚æ•°ä¼ é€’é¡ºåºï¼šå…ˆä¼ é€’å›ºå®šå‚æ•°ï¼Œå†ä¼ é€’æ‰¹æ¬¡æ•°æ®
                if processor_func(*args, batch, **kwargs):
                    success_count += 1
                    self.logger.info(f"æ‰¹æ¬¡ {batch_num}/{total_batches} å¤„ç†æˆåŠŸ ({len(batch)} æ¡è®°å½•)")
                else:
                    self.logger.error(f"æ‰¹æ¬¡ {batch_num}/{total_batches} å¤„ç†å¤±è´¥")
            except Exception as e:
                self.logger.error(f"æ‰¹æ¬¡ {batch_num}/{total_batches} å¤„ç†å¼‚å¸¸: {e}")
        
        self.logger.info(f"æ‰¹å¤„ç†å®Œæˆ: {success_count}/{total_batches} ä¸ªæ‰¹æ¬¡æˆåŠŸ")
        return success_count == total_batches
    
    # ========== ç”µå­è¡¨æ ¼ä¸“ç”¨æ–¹æ³• ==========
    
    def get_current_sheet_data(self) -> pd.DataFrame:
        """è·å–å½“å‰ç”µå­è¡¨æ ¼æ•°æ®"""
        if self.config.target_type != TargetType.SHEET:
            return pd.DataFrame()
            
        # å…ˆå°è¯•è·å–ä¸€ä¸ªåˆç†çš„èŒƒå›´
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•æ›´å°çš„èŒƒå›´ï¼Œæœ€ç»ˆè¿”å›ç©ºDataFrameè¡¨ç¤ºéœ€è¦ä½¿ç”¨cloneæ¨¡å¼
        ranges_to_try = [
            f"{self.config.sheet_id}!A1:ZZ1000",  # æœ€å¤š1000è¡Œ
            f"{self.config.sheet_id}!A1:Z500",    # æœ€å¤š500è¡Œï¼ŒZåˆ—
            f"{self.config.sheet_id}!A1:J100",    # æœ€å¤š100è¡Œï¼ŒJåˆ—
            f"{self.config.sheet_id}!A1:E50"      # æœ€å¤š50è¡Œï¼ŒEåˆ—
        ]
        
        for range_str in ranges_to_try:
            try:
                values = self.api.get_sheet_data(self.config.spreadsheet_token, range_str)
                df = self.converter.values_to_df(values)
                if not df.empty:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆæ•°æ®ï¼ˆè‡³å°‘æœ‰ä¸€è¡Œæ•°æ®åŒ…å«éç©ºå€¼ï¼‰
                    has_valid_data = False
                    for _, row in df.iterrows():
                        if any(pd.notnull(val) and str(val).strip() != '' for val in row):
                            has_valid_data = True
                            break
                    
                    if has_valid_data:
                        self.logger.info(f"æˆåŠŸè·å–ç”µå­è¡¨æ ¼æ•°æ®: {len(df)} è¡Œ x {len(df.columns)} åˆ—")
                        return df
                    else:
                        # æ•°æ®å…¨ä¸ºç©ºï¼Œå½“ä½œè¡¨æ ¼ä¸ºç©º
                        self.logger.info("ç”µå­è¡¨æ ¼æ•°æ®å…¨ä¸ºç©º")
                        return pd.DataFrame()
                else:
                    # å¦‚æœæ•°æ®ä¸ºç©ºï¼Œè¯´æ˜è¡¨æ ¼ç¡®å®æ˜¯ç©ºçš„
                    self.logger.info("ç”µå­è¡¨æ ¼ä¸ºç©º")
                    return pd.DataFrame()
            except Exception as e:
                self.logger.debug(f"å°è¯•èŒƒå›´ {range_str} å¤±è´¥: {e}")
                continue
        
        # æ‰€æœ‰èŒƒå›´éƒ½å¤±è´¥äº†ï¼Œå¯èƒ½è¡¨æ ¼æ•°æ®è¿‡å¤§ï¼Œè¿”å›ç©ºDataFrameè§¦å‘cloneæ¨¡å¼
        self.logger.warning("æ— æ³•è·å–ç”µå­è¡¨æ ¼æ•°æ®ï¼Œå¯èƒ½æ•°æ®é‡è¿‡å¤§ï¼Œå°†ä½¿ç”¨è¦†ç›–æ¨¡å¼")
        return pd.DataFrame()
    
    # ========== ç»Ÿä¸€åŒæ­¥æ–¹æ³• ==========
    
    def sync_full(self, df: pd.DataFrame) -> bool:
        """å…¨é‡åŒæ­¥ï¼šå·²å­˜åœ¨çš„æ›´æ–°ï¼Œä¸å­˜åœ¨çš„æ–°å¢"""
        self.logger.info("å¼€å§‹å…¨é‡åŒæ­¥...")
        
        if self.config.target_type == TargetType.BITABLE:
            return self._sync_full_bitable(df)
        else:  # SHEET
            return self._sync_full_sheet(df)
    
    def _sync_full_bitable(self, df: pd.DataFrame) -> bool:
        """å¤šç»´è¡¨æ ¼å…¨é‡åŒæ­¥"""
        if not self.config.index_column:
            self.logger.warning("æœªæŒ‡å®šç´¢å¼•åˆ—ï¼Œå°†æ‰§è¡Œçº¯æ–°å¢æ“ä½œ")
            field_types = self.get_field_types()
            new_records = self.converter.df_to_records(df, field_types)
            return self.process_in_batches(
                new_records, self.config.batch_size,
                self.api.batch_create_records,
                self.config.app_token, self.config.table_id
            )
        
        # è·å–ç°æœ‰è®°å½•å¹¶å»ºç«‹ç´¢å¼•
        existing_records = self.get_all_bitable_records()
        self.logger.info(f"ğŸ” è·å–åˆ°ç°æœ‰è®°å½•æ•°é‡: {len(existing_records)}")
        
        existing_index = self.converter.build_record_index(existing_records, self.config.index_column)
        self.logger.info(f"ğŸ” æ„å»ºç´¢å¼•æˆåŠŸï¼Œç´¢å¼•æ•°é‡: {len(existing_index)}")
        
        # æ‰“å°å‰å‡ ä¸ªç°æœ‰è®°å½•çš„ç´¢å¼•åˆ—å€¼ç”¨äºè°ƒè¯•
        if existing_records and len(existing_records) > 0:
            for i, record in enumerate(existing_records[:3]):
                fields = record.get('fields', {})
                index_value = fields.get(self.config.index_column, 'æœªæ‰¾åˆ°')
                self.logger.info(f"ğŸ” ç°æœ‰è®°å½• {i+1} ç´¢å¼•åˆ— '{self.config.index_column}' å€¼: '{index_value}'")
        
        field_types = self.get_field_types()
        
        # åˆ†ç±»æœ¬åœ°æ•°æ®
        records_to_update = []
        records_to_create = []
        
        for i, (_, row) in enumerate(df.iterrows()):
            index_hash = self.converter.get_index_value_hash(row, self.config.index_column)
            index_value = row.get(self.config.index_column, 'æœªæ‰¾åˆ°')
            
            # æ‰“å°å‰å‡ æ¡è®°å½•çš„åŒ¹é…ä¿¡æ¯ç”¨äºè°ƒè¯•
            if i < 3:
                self.logger.info(f"ğŸ” æ–°æ•°æ®è®°å½• {i+1} ç´¢å¼•åˆ— '{self.config.index_column}' å€¼: '{index_value}' -> å“ˆå¸Œ: {index_hash}")
                self.logger.info(f"ğŸ” å“ˆå¸Œæ˜¯å¦åœ¨ç°æœ‰ç´¢å¼•ä¸­: {index_hash in existing_index if index_hash else False}")
            
            # ä½¿ç”¨å­—æ®µç±»å‹è½¬æ¢æ„å»ºè®°å½•
            fields = {}
            for k, v in row.to_dict().items():
                if pd.notnull(v):
                    converted_value = self.converter.convert_field_value_safe(str(k), v, field_types)
                    if converted_value is not None:
                        fields[str(k)] = converted_value
            
            record = {"fields": fields}
            
            if index_hash and index_hash in existing_index:
                # éœ€è¦æ›´æ–°çš„è®°å½•
                existing_record = existing_index[index_hash]
                record["record_id"] = existing_record["record_id"]
                records_to_update.append(record)
            else:
                # éœ€è¦æ–°å¢çš„è®°å½•
                records_to_create.append(record)
        
        self.logger.info(f"å…¨é‡åŒæ­¥è®¡åˆ’: æ›´æ–° {len(records_to_update)} æ¡ï¼Œæ–°å¢ {len(records_to_create)} æ¡")
        
        # æ‰§è¡Œæ›´æ–°
        update_success = True
        if records_to_update:
            update_success = self.process_in_batches(
                records_to_update, self.config.batch_size,
                self.api.batch_update_records,
                self.config.app_token, self.config.table_id
            )
        
        # æ‰§è¡Œæ–°å¢
        create_success = True
        if records_to_create:
            create_success = self.process_in_batches(
                records_to_create, self.config.batch_size,
                self.api.batch_create_records,
                self.config.app_token, self.config.table_id
            )
        
        return update_success and create_success
    
    def _sync_full_sheet(self, df: pd.DataFrame) -> bool:
        """ç”µå­è¡¨æ ¼å…¨é‡åŒæ­¥"""
        if not self.config.index_column:
            self.logger.warning("æœªæŒ‡å®šç´¢å¼•åˆ—ï¼Œå°†æ‰§è¡Œå®Œå…¨è¦†ç›–æ“ä½œ")
            return self.sync_clone(df)
        
        # è·å–ç°æœ‰æ•°æ®
        current_df = self.get_current_sheet_data()
        
        if current_df.empty:
            self.logger.info("ç”µå­è¡¨æ ¼ä¸ºç©ºï¼Œæ‰§è¡Œæ–°å¢æ“ä½œ")
            return self.sync_clone(df)
        
        # æ„å»ºç´¢å¼•
        current_index = self.converter.build_data_index(current_df, self.config.index_column)
        
        # åˆ†ç±»æ•°æ®
        update_rows = []
        new_rows = []
        
        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(row, self.config.index_column)
            if index_hash and index_hash in current_index:
                # æ›´æ–°ç°æœ‰è¡Œ
                current_row_idx = current_index[index_hash]
                update_rows.append((current_row_idx, row))
            else:
                # æ–°å¢è¡Œ
                new_rows.append(row)
        
        self.logger.info(f"å…¨é‡åŒæ­¥è®¡åˆ’: æ›´æ–° {len(update_rows)} è¡Œï¼Œæ–°å¢ {len(new_rows)} è¡Œ")
        
        # æ‰§è¡Œæ›´æ–°
        success = True
        if update_rows:
            # æ›´æ–°ç°æœ‰è¡Œ
            updated_df = current_df.copy()
            for current_row_idx, new_row in update_rows:
                for col in df.columns:
                    if col in updated_df.columns:
                        updated_df.iloc[current_row_idx][col] = new_row[col]
            
            # å†™å…¥æ›´æ–°åçš„æ•°æ®
            values = self.converter.df_to_values(updated_df)
            success = self.api.write_sheet_data(
                self.config.spreadsheet_token, 
                self.config.sheet_id, 
                values,
                self.config.batch_size,
                80  # åˆ—æ‰¹æ¬¡å¤§å°ï¼Œä¿æŒå®‰å…¨è£•åº¦
            )
        
        # è¿½åŠ æ–°è¡Œ
        if new_rows and success:
            new_df = pd.DataFrame(new_rows)
            new_values = self.converter.df_to_values(new_df, include_headers=False)
            
            if new_values:
                self.logger.info(f"å¼€å§‹è¿½åŠ  {len(new_values)} è¡Œæ–°æ•°æ®ï¼Œä½¿ç”¨æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")
                success = self.api.append_sheet_data(
                    self.config.spreadsheet_token, 
                    self.config.sheet_id, 
                    new_values,
                    self.config.batch_size,
                    80  # åˆ—æ‰¹æ¬¡å¤§å°ï¼Œä¿æŒå®‰å…¨è£•åº¦
                )
        
        return success
    
    def sync_incremental(self, df: pd.DataFrame) -> bool:
        """å¢é‡åŒæ­¥ï¼šåªæ–°å¢ä¸å­˜åœ¨çš„è®°å½•"""
        self.logger.info("å¼€å§‹å¢é‡åŒæ­¥...")
        
        if self.config.target_type == TargetType.BITABLE:
            return self._sync_incremental_bitable(df)
        else:  # SHEET
            return self._sync_incremental_sheet(df)
    
    def _sync_incremental_bitable(self, df: pd.DataFrame) -> bool:
        """å¤šç»´è¡¨æ ¼å¢é‡åŒæ­¥"""
        if not self.config.index_column:
            self.logger.warning("æœªæŒ‡å®šç´¢å¼•åˆ—ï¼Œå°†æ‰§è¡Œçº¯æ–°å¢æ“ä½œ")
            field_types = self.get_field_types()
            new_records = self.converter.df_to_records(df, field_types)
            return self.process_in_batches(
                new_records, self.config.batch_size,
                self.api.batch_create_records,
                self.config.app_token, self.config.table_id
            )
        
        # è·å–ç°æœ‰è®°å½•å¹¶å»ºç«‹ç´¢å¼•
        existing_records = self.get_all_bitable_records()
        existing_index = self.converter.build_record_index(existing_records, self.config.index_column)
        field_types = self.get_field_types()
        
        # ç­›é€‰å‡ºéœ€è¦æ–°å¢çš„è®°å½•
        records_to_create = []
        
        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(row, self.config.index_column)
            
            if not index_hash or index_hash not in existing_index:
                # ä½¿ç”¨å­—æ®µç±»å‹è½¬æ¢æ„å»ºè®°å½•
                fields = {}
                for k, v in row.to_dict().items():
                    if pd.notnull(v):
                        converted_value = self.converter.convert_field_value_safe(str(k), v, field_types)
                        if converted_value is not None:
                            fields[str(k)] = converted_value
                
                record = {"fields": fields}
                records_to_create.append(record)
        
        self.logger.info(f"å¢é‡åŒæ­¥è®¡åˆ’: æ–°å¢ {len(records_to_create)} æ¡è®°å½•")
        
        if records_to_create:
            return self.process_in_batches(
                records_to_create, self.config.batch_size,
                self.api.batch_create_records,
                self.config.app_token, self.config.table_id
            )
        else:
            self.logger.info("æ²¡æœ‰æ–°è®°å½•éœ€è¦åŒæ­¥")
            return True
    
    def _sync_incremental_sheet(self, df: pd.DataFrame) -> bool:
        """ç”µå­è¡¨æ ¼å¢é‡åŒæ­¥"""
        if not self.config.index_column:
            self.logger.warning("æœªæŒ‡å®šç´¢å¼•åˆ—ï¼Œå°†æ–°å¢å…¨éƒ¨æ•°æ®")
            # è¿½åŠ æ‰€æœ‰æ•°æ®
            values = self.converter.df_to_values(df)
            self.logger.info(f"æœªæŒ‡å®šç´¢å¼•åˆ—ï¼Œå¼€å§‹è¿½åŠ å…¨éƒ¨ {len(values)} è¡Œæ•°æ®ï¼Œä½¿ç”¨æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")
            return self.api.append_sheet_data(
                self.config.spreadsheet_token, 
                self.config.sheet_id, 
                values,
                self.config.batch_size,
                80  # åˆ—æ‰¹æ¬¡å¤§å°ï¼Œä¿æŒå®‰å…¨è£•åº¦
            )
        
        # è·å–ç°æœ‰æ•°æ®
        current_df = self.get_current_sheet_data()
        
        if current_df.empty:
            self.logger.info("ç”µå­è¡¨æ ¼ä¸ºç©ºï¼Œæ–°å¢å…¨éƒ¨æ•°æ®")
            # ä½¿ç”¨å…‹éš†åŒæ­¥ï¼ˆä¼šå…ˆå†™å…¥æ•°æ®å†è®¾ç½®æ ¼å¼ï¼‰
            return self.sync_clone(df)
        
        # æ„å»ºç´¢å¼•
        current_index = self.converter.build_data_index(current_df, self.config.index_column)
        
        # ç­›é€‰éœ€è¦æ–°å¢çš„è®°å½•
        new_rows = []
        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(row, self.config.index_column)
            if not index_hash or index_hash not in current_index:
                new_rows.append(row)
        
        self.logger.info(f"å¢é‡åŒæ­¥è®¡åˆ’: æ–°å¢ {len(new_rows)} è¡Œ")
        
        if new_rows:
            new_df = pd.DataFrame(new_rows)
            new_values = self.converter.df_to_values(new_df, include_headers=False)
            
            # è¿½åŠ æ–°æ•°æ®
            self.logger.info(f"å¼€å§‹å¢é‡è¿½åŠ  {len(new_values)} è¡Œæ•°æ®ï¼Œä½¿ç”¨æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")
            return self.api.append_sheet_data(
                self.config.spreadsheet_token, 
                self.config.sheet_id, 
                new_values,
                self.config.batch_size,
                80  # åˆ—æ‰¹æ¬¡å¤§å°ï¼Œä¿æŒå®‰å…¨è£•åº¦
            )
        else:
            self.logger.info("æ²¡æœ‰æ–°è®°å½•éœ€è¦åŒæ­¥")
            return True
    
    def sync_overwrite(self, df: pd.DataFrame) -> bool:
        """è¦†ç›–åŒæ­¥ï¼šåˆ é™¤å·²å­˜åœ¨çš„ï¼Œç„¶åæ–°å¢å…¨éƒ¨"""
        self.logger.info("å¼€å§‹è¦†ç›–åŒæ­¥...")
        
        if not self.config.index_column:
            self.logger.error("è¦†ç›–åŒæ­¥æ¨¡å¼éœ€è¦æŒ‡å®šç´¢å¼•åˆ—")
            return False
        
        if self.config.target_type == TargetType.BITABLE:
            return self._sync_overwrite_bitable(df)
        else:  # SHEET
            return self._sync_overwrite_sheet(df)
    
    def _sync_overwrite_bitable(self, df: pd.DataFrame) -> bool:
        """å¤šç»´è¡¨æ ¼è¦†ç›–åŒæ­¥"""
        # è·å–ç°æœ‰è®°å½•å¹¶å»ºç«‹ç´¢å¼•
        existing_records = self.get_all_bitable_records()
        existing_index = self.converter.build_record_index(existing_records, self.config.index_column)
        field_types = self.get_field_types()
        
        # æ‰¾å‡ºéœ€è¦åˆ é™¤çš„è®°å½•
        record_ids_to_delete = []
        
        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(row, self.config.index_column)
            if index_hash and index_hash in existing_index:
                existing_record = existing_index[index_hash]
                record_ids_to_delete.append(existing_record["record_id"])
        
        self.logger.info(f"è¦†ç›–åŒæ­¥è®¡åˆ’: åˆ é™¤ {len(record_ids_to_delete)} æ¡å·²å­˜åœ¨è®°å½•ï¼Œç„¶åæ–°å¢ {len(df)} æ¡è®°å½•")
        
        # åˆ é™¤å·²å­˜åœ¨çš„è®°å½•
        delete_success = True
        if record_ids_to_delete:
            delete_success = self.process_in_batches(
                record_ids_to_delete, self.config.batch_size,
                self.api.batch_delete_records,
                self.config.app_token, self.config.table_id
            )
        
        # æ–°å¢å…¨éƒ¨è®°å½•
        new_records = self.converter.df_to_records(df, field_types)
        create_success = self.process_in_batches(
            new_records, self.config.batch_size,
            self.api.batch_create_records,
            self.config.app_token, self.config.table_id
        )
        
        return delete_success and create_success
    
    def _sync_overwrite_sheet(self, df: pd.DataFrame) -> bool:
        """ç”µå­è¡¨æ ¼è¦†ç›–åŒæ­¥"""
        # è·å–ç°æœ‰æ•°æ®
        current_df = self.get_current_sheet_data()
        
        if current_df.empty:
            self.logger.info("ç”µå­è¡¨æ ¼ä¸ºç©ºï¼Œæ‰§è¡Œæ–°å¢æ“ä½œ")
            return self.sync_clone(df)
        
        # æ‰¾å‡ºéœ€è¦åˆ é™¤çš„è®°å½•å¹¶æ„å»ºæ–°çš„æ•°æ®é›†
        new_df_rows = []
        deleted_count = 0
        
        # ä¿ç•™ä¸åœ¨æ–°æ•°æ®ä¸­çš„ç°æœ‰è®°å½•
        for _, row in current_df.iterrows():
            index_hash = self.converter.get_index_value_hash(row, self.config.index_column)
            if index_hash:
                # æ£€æŸ¥æ˜¯å¦åœ¨æ–°æ•°æ®ä¸­
                found_in_new = False
                for _, new_row in df.iterrows():
                    new_index_hash = self.converter.get_index_value_hash(new_row, self.config.index_column)
                    if new_index_hash == index_hash:
                        found_in_new = True
                        break
                
                if not found_in_new:
                    new_df_rows.append(row)
                else:
                    deleted_count += 1
        
        # æ·»åŠ æ–°æ•°æ®
        for _, row in df.iterrows():
            new_df_rows.append(row)
        
        self.logger.info(f"è¦†ç›–åŒæ­¥è®¡åˆ’: åˆ é™¤ {deleted_count} è¡Œï¼Œæ–°å¢ {len(df)} è¡Œ")
        
        # é‡å†™æ•´ä¸ªè¡¨æ ¼
        if new_df_rows:
            new_df = pd.DataFrame(new_df_rows)
            values = self.converter.df_to_values(new_df)
            
            # å…ˆæ¸…ç©ºç°æœ‰æ•°æ®ï¼Œç„¶åå†™å…¥æ–°æ•°æ®
            return self.api.write_sheet_data(
                self.config.spreadsheet_token, 
                self.config.sheet_id, 
                values,
                self.config.batch_size,
                80  # åˆ—æ‰¹æ¬¡å¤§å°ï¼Œä¿æŒå®‰å…¨è£•åº¦
            )
        else:
            # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œæ¸…ç©ºè¡¨æ ¼
            return self.api.clear_sheet_data(self.config.spreadsheet_token, f"{self.config.sheet_id}!A:Z")
    
    def sync_clone(self, df: pd.DataFrame) -> bool:
        """å…‹éš†åŒæ­¥ï¼šæ¸…ç©ºå…¨éƒ¨ï¼Œç„¶åæ–°å¢å…¨éƒ¨"""
        self.logger.info("å¼€å§‹å…‹éš†åŒæ­¥...")
        
        if self.config.target_type == TargetType.BITABLE:
            return self._sync_clone_bitable(df)
        else:  # SHEET
            return self._sync_clone_sheet(df)
    
    def _sync_clone_bitable(self, df: pd.DataFrame) -> bool:
        """å¤šç»´è¡¨æ ¼å…‹éš†åŒæ­¥"""
        # è·å–æ‰€æœ‰ç°æœ‰è®°å½•
        existing_records = self.get_all_bitable_records()
        existing_record_ids = [record["record_id"] for record in existing_records]
        
        self.logger.info(f"å…‹éš†åŒæ­¥è®¡åˆ’: åˆ é™¤ {len(existing_record_ids)} æ¡å·²æœ‰è®°å½•ï¼Œç„¶åæ–°å¢ {len(df)} æ¡è®°å½•")
        
        # åˆ é™¤æ‰€æœ‰è®°å½•
        delete_success = True
        if existing_record_ids:
            delete_success = self.process_in_batches(
                existing_record_ids, self.config.batch_size,
                self.api.batch_delete_records,
                self.config.app_token, self.config.table_id
            )
        
        # æ–°å¢å…¨éƒ¨è®°å½•
        field_types = self.get_field_types()
        new_records = self.converter.df_to_records(df, field_types)
        create_success = self.process_in_batches(
            new_records, self.config.batch_size,
            self.api.batch_create_records,
            self.config.app_token, self.config.table_id
        )
        
        return delete_success and create_success
    
    def _sync_clone_sheet(self, df: pd.DataFrame) -> bool:
        """ç”µå­è¡¨æ ¼å…‹éš†åŒæ­¥"""
        # è½¬æ¢æ•°æ®æ ¼å¼
        values = self.converter.df_to_values(df)
        
        self.logger.info(f"å…‹éš†åŒæ­¥è®¡åˆ’: æ¸…ç©ºç°æœ‰æ•°æ®ï¼Œæ–°å¢ {len(df)} è¡Œ")
        self.logger.info(f"ä½¿ç”¨é…ç½®çš„æ‰¹æ¬¡å¤§å°: {self.config.batch_size} è¡Œ/æ‰¹")
        
        # ä½¿ç”¨äºŒç»´åˆ†å—å†™å…¥æ•°æ®ï¼ˆä¼šè¦†ç›–ç°æœ‰æ•°æ®ï¼‰
        write_success = self.api.write_sheet_data(
            self.config.spreadsheet_token, 
            self.config.sheet_id, 
            values,
            self.config.batch_size,
            80  # åˆ—æ‰¹æ¬¡å¤§å°ï¼Œä¿æŒå®‰å…¨è£•åº¦
        )
        
        # æ•°æ®å†™å…¥æˆåŠŸåï¼Œå†åº”ç”¨æ™ºèƒ½å­—æ®µé…ç½®
        if write_success:
            if not self._setup_sheet_intelligence(df):
                self.logger.warning("æ™ºèƒ½å­—æ®µé…ç½®å¤±è´¥ï¼Œä½†æ•°æ®åŒæ­¥å·²å®Œæˆ")
        
        return write_success
    
    def _setup_sheet_intelligence(self, df: pd.DataFrame) -> bool:
        """
        ä¸ºç”µå­è¡¨æ ¼è®¾ç½®æ™ºèƒ½å­—æ®µé…ç½®
        
        Args:
            df: æ•°æ®DataFrame
            
        Returns:
            æ˜¯å¦è®¾ç½®æˆåŠŸ
        """
        if self.config.target_type != TargetType.SHEET:
            return True
        
        # ä¸åŒç­–ç•¥çš„é…ç½®èŒƒå›´ä¸åŒ
        strategy_name = self.config.field_type_strategy.value
        self.logger.info(f"å¼€å§‹ç”µå­è¡¨æ ¼æ™ºèƒ½å­—æ®µé…ç½® ({strategy_name}ç­–ç•¥)...")
        
        # rawç­–ç•¥ï¼šä¸åº”ç”¨ä»»ä½•æ ¼å¼åŒ–ï¼Œç›´æ¥è¿”å›æˆåŠŸ
        if strategy_name == 'raw':
            self.logger.info("rawç­–ç•¥ï¼šè·³è¿‡æ‰€æœ‰æ ¼å¼åŒ–ï¼Œä¿æŒåŸå§‹æ•°æ®")
            return True
        
        # ç”Ÿæˆå­—æ®µé…ç½®
        field_config = self.converter.generate_sheet_field_config(
            df, self.config.field_type_strategy.value, self.config
        )
        
        success = True
        
        # 1. é…ç½®ä¸‹æ‹‰åˆ—è¡¨ (baseç­–ç•¥è·³è¿‡)
        if strategy_name != 'base':
            for dropdown_config in field_config['dropdown_configs']:
                column_name = dropdown_config['column']
                
                # è®¡ç®—åˆ—ç´¢å¼•
                col_index = list(df.columns).index(column_name)
                col_letter = self.converter.column_number_to_letter(col_index + 1)
                
                # è®¾ç½®ä¸‹æ‹‰åˆ—è¡¨èŒƒå›´ (ä»ç¬¬2è¡Œå¼€å§‹ï¼Œé¿å…è¦†ç›–æ ‡é¢˜)
                actual_end_row = len(df) + 1  # +1 for header row
                range_str = f"{self.config.sheet_id}!{col_letter}2:{col_letter}{actual_end_row}"
                
                # ç¡®ä¿ä½¿ç”¨SheetAPIå¹¶æ£€æŸ¥token
                if not isinstance(self.api, SheetAPI):
                    self.logger.error("APIç±»å‹ä¸åŒ¹é…ï¼Œéœ€è¦SheetAPI")
                    continue
                    
                if not self.config.spreadsheet_token:
                    self.logger.error("ç”µå­è¡¨æ ¼Tokenä¸ºç©º")
                    continue
                
                # è®¾ç½®ä¸‹æ‹‰åˆ—è¡¨
                dropdown_success = self.api.set_dropdown_validation(
                    self.config.spreadsheet_token,
                    range_str,
                    dropdown_config['options'],
                    dropdown_config['multiple'],
                    dropdown_config['colors']
                )
                
                if dropdown_success:
                    self.logger.info(f"æˆåŠŸä¸ºåˆ— '{column_name}' è®¾ç½®ä¸‹æ‹‰åˆ—è¡¨")
                else:
                    self.logger.error(f"ä¸ºåˆ— '{column_name}' è®¾ç½®ä¸‹æ‹‰åˆ—è¡¨å¤±è´¥")
                    # ä¸è®¾ç½®success = Falseï¼Œå…è®¸ç»§ç»­å…¶ä»–åˆ—çš„æ“ä½œ
        else:
            self.logger.info("baseç­–ç•¥è·³è¿‡ä¸‹æ‹‰åˆ—è¡¨é…ç½®")
        
        # 2. é…ç½®æ—¥æœŸæ ¼å¼
        if field_config['date_columns'] and isinstance(self.api, SheetAPI) and self.config.spreadsheet_token:
            date_ranges = []
            for column_name in field_config['date_columns']:
                col_index = list(df.columns).index(column_name)
                col_letter = self.converter.column_number_to_letter(col_index + 1)
                actual_end_row = len(df) + 1  # +1 for header row
                range_str = f"{self.config.sheet_id}!{col_letter}2:{col_letter}{actual_end_row}"
                date_ranges.append(range_str)
            
            # è®¾ç½®æ—¥æœŸæ ¼å¼
            date_success = self.api.set_date_format(
                self.config.spreadsheet_token,
                date_ranges,
                "yyyy/MM/dd"
            )
            
            if date_success:
                self.logger.info(f"æˆåŠŸä¸º {len(date_ranges)} ä¸ªæ—¥æœŸåˆ—è®¾ç½®æ ¼å¼")
            else:
                self.logger.error("è®¾ç½®æ—¥æœŸæ ¼å¼å¤±è´¥")
                # ä¸è®¾ç½®success = Falseï¼Œå…è®¸ç»§ç»­å…¶ä»–æ“ä½œ
        
        # 3. é…ç½®æ•°å­—æ ¼å¼
        if field_config['number_columns'] and isinstance(self.api, SheetAPI) and self.config.spreadsheet_token:
            number_ranges = []
            for column_name in field_config['number_columns']:
                col_index = list(df.columns).index(column_name)
                col_letter = self.converter.column_number_to_letter(col_index + 1)
                actual_end_row = len(df) + 1  # +1 for header row
                range_str = f"{self.config.sheet_id}!{col_letter}2:{col_letter}{actual_end_row}"
                number_ranges.append(range_str)
            
            # è®¾ç½®æ•°å­—æ ¼å¼
            number_success = self.api.set_number_format(
                self.config.spreadsheet_token,
                number_ranges,
                "#,##0.00"
            )
            
            if number_success:
                self.logger.info(f"æˆåŠŸä¸º {len(number_ranges)} ä¸ªæ•°å­—åˆ—è®¾ç½®æ ¼å¼")
            else:
                self.logger.error("è®¾ç½®æ•°å­—æ ¼å¼å¤±è´¥")
                # ä¸è®¾ç½®success = Falseï¼Œå…è®¸ç»§ç»­å…¶ä»–æ“ä½œ
        
        # è¾“å‡ºé…ç½®æ‘˜è¦
        dropdown_count = len(field_config['dropdown_configs']) if strategy_name != 'base' else 0
        date_count = len(field_config['date_columns'])
        number_count = len(field_config['number_columns'])
        total_configs = dropdown_count + date_count + number_count
        
        if total_configs > 0:
            config_summary = []
            if dropdown_count > 0:
                config_summary.append(f"{dropdown_count}ä¸ªä¸‹æ‹‰åˆ—è¡¨")
            if date_count > 0:
                config_summary.append(f"{date_count}ä¸ªæ—¥æœŸæ ¼å¼")
            if number_count > 0:
                config_summary.append(f"{number_count}ä¸ªæ•°å­—æ ¼å¼")
            
            self.logger.info(f"æ™ºèƒ½å­—æ®µé…ç½®å®Œæˆ: {', '.join(config_summary)}")
        else:
            self.logger.info("æœªæ£€æµ‹åˆ°éœ€è¦æ™ºèƒ½é…ç½®çš„å­—æ®µ")
        
        return success
    
    def sync(self, df: pd.DataFrame) -> bool:
        """æ‰§è¡ŒåŒæ­¥"""
        target_name = "å¤šç»´è¡¨æ ¼" if self.config.target_type == TargetType.BITABLE else "ç”µå­è¡¨æ ¼"
        self.logger.info(f"å¼€å§‹æ‰§è¡Œ {target_name} {self.config.sync_mode.value} åŒæ­¥æ¨¡å¼")
        self.logger.info(f"æ•°æ®æº: {len(df)} è¡Œ x {len(df.columns)} åˆ—")
        
        # é‡ç½®è½¬æ¢ç»Ÿè®¡
        self.converter.reset_stats()
        
        # å¤šç»´è¡¨æ ¼æ¨¡å¼éœ€è¦ç¡®ä¿å­—æ®µå­˜åœ¨
        if self.config.target_type == TargetType.BITABLE:
            success, field_types = self.ensure_fields_exist(df)
            if not success:
                self.logger.error("å­—æ®µåˆ›å»ºå¤±è´¥ï¼ŒåŒæ­¥ç»ˆæ­¢")
                return False
            
            self.logger.info(f"è·å–åˆ° {len(field_types)} ä¸ªå­—æ®µçš„ç±»å‹ä¿¡æ¯")
            
            # æ˜¾ç¤ºå­—æ®µç±»å‹æ˜ å°„æ‘˜è¦
            self._show_field_analysis_summary(df, field_types)
            
            # é¢„æ£€æŸ¥ï¼šåˆ†ææ•°æ®ä¸å­—æ®µç±»å‹çš„åŒ¹é…æƒ…å†µ
            self.logger.info("\nğŸ” æ­£åœ¨åˆ†ææ•°æ®ä¸å­—æ®µç±»å‹åŒ¹é…æƒ…å†µ...")
            mismatch_warnings = []
            sample_size = min(50, len(df))  # æ£€æŸ¥å‰50è¡Œä½œä¸ºæ ·æœ¬
            
            for _, row in df.head(sample_size).iterrows():
                for col_name, value in row.to_dict().items():
                    if pd.notnull(value) and col_name in field_types:
                        field_type = field_types[col_name]
                        # ç®€å•çš„ç±»å‹ä¸åŒ¹é…æ£€æµ‹
                        if field_type == 2 and isinstance(value, str):  # æ•°å­—å­—æ®µä½†æ˜¯å­—ç¬¦ä¸²å€¼
                            if not self.converter._is_number_string(str(value).strip()):
                                mismatch_warnings.append(f"å­—æ®µ '{col_name}' æ˜¯æ•°å­—ç±»å‹ï¼Œä½†åŒ…å«éæ•°å­—å€¼: '{value}'")
                        elif field_type == 5 and isinstance(value, str):  # æ—¥æœŸå­—æ®µä½†æ˜¯å­—ç¬¦ä¸²å€¼
                            if not (self.converter._is_timestamp_string(str(value)) or self.converter._is_date_string(str(value))):
                                mismatch_warnings.append(f"å­—æ®µ '{col_name}' æ˜¯æ—¥æœŸç±»å‹ï¼Œä½†åŒ…å«éæ—¥æœŸå€¼: '{value}'")
            
            if mismatch_warnings:
                unique_warnings = list(set(mismatch_warnings[:10]))  # æ˜¾ç¤ºå‰10ä¸ªå”¯ä¸€è­¦å‘Š
                self.logger.warning(f"å‘ç° {len(set(mismatch_warnings))} ç§æ•°æ®ç±»å‹ä¸åŒ¹é…æƒ…å†µï¼ˆæ ·æœ¬æ£€æŸ¥ï¼‰:")
                for warning in unique_warnings:
                    self.logger.warning(f"  â€¢ {warning}")
                self.logger.info("ç¨‹åºå°†è‡ªåŠ¨è¿›è¡Œå¼ºåˆ¶ç±»å‹è½¬æ¢...")
            else:
                self.logger.info("âœ… æ•°æ®ç±»å‹åŒ¹é…è‰¯å¥½")
        
        # æ ¹æ®åŒæ­¥æ¨¡å¼æ‰§è¡Œå¯¹åº”æ“ä½œ
        sync_result = False
        if self.config.sync_mode == SyncMode.FULL:
            sync_result = self.sync_full(df)
        elif self.config.sync_mode == SyncMode.INCREMENTAL:
            sync_result = self.sync_incremental(df)
        elif self.config.sync_mode == SyncMode.OVERWRITE:
            sync_result = self.sync_overwrite(df)
        elif self.config.sync_mode == SyncMode.CLONE:
            sync_result = self.sync_clone(df)
        else:
            self.logger.error(f"ä¸æ”¯æŒçš„åŒæ­¥æ¨¡å¼: {self.config.sync_mode}")
            return False
        
        # è¾“å‡ºè½¬æ¢ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…å¤šç»´è¡¨æ ¼æ¨¡å¼ï¼‰
        if self.config.target_type == TargetType.BITABLE:
            self.converter.report_conversion_stats()
        
        return sync_result
    
    def _show_field_analysis_summary(self, df: pd.DataFrame, field_types: Dict[str, int]):
        """æ˜¾ç¤ºå­—æ®µåˆ†ææ‘˜è¦"""
        self.logger.info("\nğŸ“‹ å­—æ®µç±»å‹æ˜ å°„æ‘˜è¦:")
        self.logger.info("-" * 50)
        
        for col_name in df.columns:
            if col_name in field_types:
                field_type = field_types[col_name]
                type_name = self.converter.get_field_type_name(field_type)
                self.logger.info(f"  {col_name} â†’ {type_name} (ç±»å‹ç : {field_type})")
            else:
                self.logger.warning(f"  {col_name} â†’ æœªçŸ¥å­—æ®µç±»å‹")
                
        self.logger.info("-" * 50)